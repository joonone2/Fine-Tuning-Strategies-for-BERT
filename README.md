# BERT Fine-Tuning Strategies for SST-2

ë³¸ í”„ë¡œì íŠ¸ëŠ” **BERT ê¸°ë°˜ ê°ì • ë¶„ë¥˜(SST-2)** ë¬¸ì œë¥¼ ëŒ€ìƒìœ¼ë¡œ, ë‹¤ì–‘í•œ **íŒŒì¸íŠœë‹(Fine-Tuning)** ì „ëµì„ ë¹„êµí•œ ì—°êµ¬ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.  
ëª¨ë“  ì‹¤í—˜ì€ ë™ì¼í•œ **BERT-base-uncased ë°±ë³¸**, **ë™ì¼í•œ ë°ì´í„° ë¶„í•  (Train/Validation/Test = 8:1:1)**, **ë™ì¼í•œ í‰ê°€ ì§€í‘œ**ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³µì •í•˜ê²Œ ë¹„êµí•˜ì˜€ìŠµë‹ˆë‹¤.

------------------------------------------------------------

## 1. ë¹„êµí•œ Fine-Tuning ê¸°ë²•

ì•„ë˜ëŠ” ë³¸ í”„ë¡œì íŠ¸ì—ì„œ ì‹¤í—˜í•œ íŒŒì¸íŠœë‹ ë°©ì‹ê³¼ ê° ëª¨ë¸ì˜ **í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜(Trainable Params)** ì •ë¦¬ì…ë‹ˆë‹¤.

Model ID | Backbone | Tuning Strategy | Trainable Params
-------- | -------- | --------------- | ------------------------------
M1 | bert-base-uncased | Head-only (CLS Linear Only) | 1,538 / 109,483,778 (0.0014%)
M2 | bert-base-uncased | Full Fine-tuning | 109,483,778 / 109,483,778 (100%)
M3 | bert-base-uncased | Partial FT (Top-4 layers) | 28,353,026 / 109,483,778 (25.897%)
M4 | bert-base-uncased | BitFit (Bias-only Tuning) | 102,914 / 109,483,778 (0.094%)
M5 | bert-base-uncased | LoRA (r = 4 / 8 / 16) | ì•½ 0.1% ~ 1%



------------------------------------------------------------

## 2. ë°ì´í„°ì…‹ ì •ë³´

â€¢ ë°ì´í„°ì…‹: SST-2 (Stanford Sentiment Treebank v2)  
â€¢ Train ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ **8:1:1 ë¹„ìœ¨ë¡œ** ì¬ë¶„í•   
â€¢ ëª¨ë“  ìˆ˜í–‰ ë…¸íŠ¸ë¶ì—ì„œ ë™ì¼í•œ ë°ì´í„° ë¶„í• ì„ ì ìš©í•˜ì—¬ ì¬í˜„ì„± í™•ë³´


------------------------------------------------------------

## 3. í”„ë¡œì íŠ¸ í´ë” êµ¬ì¡°

## ğŸ“ í”„ë¡œì íŠ¸ í´ë” êµ¬ì¡°

```
project/
â”‚
â”œâ”€â”€ models/                     # ëª¨ë¸ ì €ì¥ í´ë”
â”‚   â”œâ”€â”€ freeze_model/           # Head-only fine-tuning ê²°ê³¼ ëª¨ë¸
â”‚   â”œâ”€â”€ full_fine_model/        # Full fine-tuning ê²°ê³¼ ëª¨ë¸
â”‚   â”œâ”€â”€ partial_ft_model/       # Partial (Top-k layers) fine-tuning ëª¨ë¸
â”‚   â”œâ”€â”€ bitfit_model/           # Bias-only (BitFit) fine-tuning ëª¨ë¸
â”‚   â””â”€â”€ lora_model/             # LoRA fine-tuning ëª¨ë¸
â”‚
â”œâ”€â”€ results/                    # í…ŒìŠ¤íŠ¸ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ CSV íŒŒì¼
â”‚   â”œâ”€â”€ freeze_test_outputs.csv
â”‚   â”œâ”€â”€ full_test_outputs.csv
â”‚   â”œâ”€â”€ partial_test_outputs.csv
â”‚   â”œâ”€â”€ bitfit_test_outputs.csv
â”‚   â””â”€â”€ lora_test_outputs.csv
â”‚
â”œâ”€â”€ data/                       # SST-2 ì›ë³¸ ë°ì´í„° ì €ì¥ í´ë”
â”‚   â””â”€â”€ sst2_raw/
â”‚
â”œâ”€â”€ notebooks/                  # ê°œë³„ ì‹¤í—˜ì„ ì‹¤í–‰ Notebook íŒŒì¼
â”‚   â”œâ”€â”€ freeze.ipynb            # Freeze(Head-only) ì‹¤í—˜
â”‚   â”œâ”€â”€ full_fine.ipynb         # Full fine-tuning ì‹¤í—˜
â”‚   â”œâ”€â”€ partial_ft.ipynb        # Partial fine-tuning ì‹¤í—˜
â”‚   â”œâ”€â”€ bitfit.ipynb            # BitFit ì‹¤í—˜
â”‚   â”œâ”€â”€ lora.ipynb              # LoRA ì‹¤í—˜
â”‚   â””â”€â”€ model_test.ipynb        # ëª¨ë“  ëª¨ë¸ì„ ë™ì¼í•œ testsetìœ¼ë¡œ í‰ê°€
â”‚
â”œâ”€â”€ README.md                   
â””â”€â”€ requirements.txt           
```

------------------------------------------------------------

## 4. í´ë” ë° íŒŒì¼ ì„¤ëª…

ğŸ“ **models/**  
â€¢ GitHubì—ëŠ” íŒŒì¼ ìš©ëŸ‰ ë¬¸ì œë¡œ ë¹„ì›Œë‘¡ë‹ˆë‹¤.  
â€¢ Hugging Face Hubì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•´ ì´ í´ë”ì— ë°°ì¹˜í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.  
â€¢ ê° ë””ë ‰í† ë¦¬ëŠ” í•´ë‹¹ íŒŒì¸íŠœë‹ ë°©ì‹ì˜ ëª¨ë¸ì„ ì €ì¥í•˜ê¸° ìœ„í•œ í´ë”ì…ë‹ˆë‹¤.

ğŸ“ **results/**  
â€¢ ê° ëª¨ë¸ì„ ë™ì¼í•œ test splitì—ì„œ í‰ê°€í•œ ê²°ê³¼(csv)ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.  
â€¢ CSV ì»¬ëŸ¼: `sentence, gold, pred`

ğŸ“ **notebooks/  
ê° ì‹¤í—˜ì€ ë…ë¦½ì ì¸ Jupyter Notebookìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

Notebook | ì„¤ëª…
-------- | ----
freeze.ipynb | BERT Encoder ë™ê²° + Linear Classifierë§Œ í•™ìŠµ
full_fine.ipynb | BERT ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµ
partial_ft.ipynb | ë§ˆì§€ë§‰ Kê°œì˜ encoder layerë§Œ í•™ìŠµ
bitfit.ipynb | Bias-only íŠœë‹ (BitFit)
lora.ipynb | LoRA íŠœë‹ (r = 4, 8, 16 ë“±)
model_test.ipynb | ì €ì¥ëœ ëª¨ë“  ëª¨ë¸ì„ ë™ì¼í•œ test setì— ëŒ€í•´ í‰ê°€

------------------------------------------------------------

## 5. ì‹¤í–‰ ë°©ë²•

### 1) íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
pip install -r requirements.txt
```
### 2) ê°œë³„ ì‹¤í—˜ ìˆ˜í–‰
ì˜ˆ: Head-only Fine-Tuning â†’ `notebooks/freeze.ipynb`

### 3) ì €ì¥ëœ ëª¨ë¸ í‰ê°€
`notebooks/model_test.ipynb` ì‹¤í–‰

------------------------------------------------------------

## 6. Hugging Face Hub ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

ê° íŒŒì¸íŠœë‹ ë°©ì‹ì˜ ëª¨ë¸ì€ Hugging Face Hubì— ì—…ë¡œë“œë˜ì–´ ìˆìœ¼ë©°,  
ë‹¤ìš´ë¡œë“œ í›„ `models/` í´ë” ë‚´ë¶€ì— ë°°ì¹˜í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.

freeze_model:     https://huggingface.co/joononeyyy/freeze-sst2  
full_fine_model:  https://huggingface.co/joononeyyy/full-sst2  
partial_ft_model: https://huggingface.co/joononeyyy/partial-sst2  
bitfit_model:     https://huggingface.co/joononeyyy/bitfit-sst2  
lora_model:       https://huggingface.co/joononeyyy/lora-sst2  

------------------------------------------------------------

## 7. í‰ê°€ ì§€í‘œ

ëª¨ë“  ëª¨ë¸ì€ ë™ì¼í•œ í‰ê°€ ì§€í‘œë¡œ ì„±ëŠ¥ì„ ë¹„êµí•˜ì˜€ìŠµë‹ˆë‹¤.

â€¢ Accuracy  
â€¢ Precision (macro)  
â€¢ Recall (macro)  
â€¢ F1-score (macro)

------------------------------------------------------------

## 8. ì„±ëŠ¥ ìš”ì•½
| Model Name        | Fine-tuning Strategy        | Trainable Params (%)       | Test F1  |
|-------------------|-----------------------------|-----------------------------|----------|
| Freeze FT         | Head-only (Classifier only) | 0.0014%                     | 0.74  |
| Full Fine-tune     | Full Parameter Training     | 100%                        | 0.96   |
| Partial FT         | Top-4 Layers Only           | 25.897%                     | 0.94   |
| BitFit             | Bias-only Training          | 0.094%                      | 0.93   |
