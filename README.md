# BERT Fine-Tuning Strategies for SST-2

ë³¸ í”„ë¡œì íŠ¸ëŠ” **BERT ê¸°ë°˜ ê°ì • ë¶„ë¥˜(SST-2)** ë¬¸ì œë¥¼ ëŒ€ìƒìœ¼ë¡œ, ë‹¤ì–‘í•œ **íŒŒì¸íŠœë‹(Fine-Tuning)** ì „ëµì„ ë¹„êµí•œ ì—°êµ¬ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.  
ëª¨ë“  ì‹¤í—˜ì€ ë™ì¼í•œ **BERT-base-uncased ë°±ë³¸**, **ë™ì¼í•œ ë°ì´í„° ë¶„í•  (Train/Validation/Test = 8:1:1)**, **ë™ì¼í•œ í‰ê°€ ì§€í‘œ**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„êµí•˜ì˜€ìŠµë‹ˆë‹¤.

------------------------------------------------------------

## 1. ë¹„êµí•œ Fine-Tuning ê¸°ë²•

ì•„ë˜ëŠ” ë³¸ í”„ë¡œì íŠ¸ì—ì„œ ì‹¤í—˜í•œ íŒŒì¸íŠœë‹ ë°©ì‹ê³¼ ê° ëª¨ë¸ì˜ **í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜(Trainable Params)** ì •ë¦¬ì…ë‹ˆë‹¤.

| Model Name        | Backbone             | Fine-tuning Strategy        | Trainable Params (%) |
|-------------------|-----------------------|-----------------------------|-----------------------|
| Freeze FT         | bert-base-uncased     | Head-only (Classifier only) | 0.0014%               |
| Full Fine-tune     | bert-base-uncased     | Full Parameter Training     | 100%                  |
| Partial FT         | bert-base-uncased     | Top-4 Layers Only           | 25.897%               |
| BitFit             | bert-base-uncased     | Bias-only Training          | 0.094%                |
| LoRA               | bert-base-uncased     | Low-Rank Adaptation (r=8)   | 1.209956%             |



------------------------------------------------------------

## 2. ë°ì´í„°ì…‹ ì •ë³´

â€¢ ë°ì´í„°ì…‹: SST-2 (Stanford Sentiment Treebank v2)  
â€¢ Train ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ **8:1:1 ë¹„ìœ¨ë¡œ** ì¬ë¶„í•   
â€¢ ëª¨ë“  ìˆ˜í–‰ ë…¸íŠ¸ë¶ì—ì„œ ë™ì¼í•œ ë°ì´í„° ë¶„í• ì„ ì ìš©í•˜ì—¬ ì¬í˜„ì„± í™•ë³´


------------------------------------------------------------
## 3. ì‹¤í—˜ ë°©ë²• (Experiment Method)

ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” ëª¨ë“  íŒŒì¸íŠœë‹ ê¸°ë²•ì„ ê³µì •í•˜ê²Œ ë¹„êµí•˜ê¸° ìœ„í•´, ë™ì¼í•œ ë°ì´í„° ë¶„í• , ë™ì¼í•œ ë°±ë³¸(BERT-base-uncased), ë™ì¼í•œ í‰ê°€ ë°©ì‹(F1-score ì¤‘ì‹¬)ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” ì „ì²´ ì‹¤í—˜ ì ˆì°¨ì…ë‹ˆë‹¤.

---

### 1) ë°ì´í„° êµ¬ì„±
- Hugging Face SST-2 ë°ì´í„°ì…‹ì˜ **train splitë§Œ ë¡œë“œ**
- 8 : 1 : 1 ë¹„ìœ¨ë¡œ **train / validation / test** ì¬êµ¬ì„±
- ëª¨ë“  ì‹¤í—˜ì—ì„œ ë™ì¼í•œ ë¶„í•  ì‚¬ìš©

---

### 2) í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
ê° íŒŒì¸íŠœë‹ ë°©ì‹ì€ **Random Search**ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì  ì„¤ì •ì„ íƒìƒ‰í–ˆìŠµë‹ˆë‹¤.

íƒìƒ‰ ë²”ìœ„:
- learning rate: 2e-5, 3e-5, 5e-5
- batch size: 16, 32, 64
- dropout: 0.1, 0.2
- epochs: 2, 3, 4

ì—¬ëŸ¬ trial ì¤‘ **validation Accê°€ ê°€ì¥ ë†’ì€ ëª¨ë¸**ì„ ìµœì¢… ê²°ê³¼ ë¹„êµì— ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

---

### 3) ê¸°ë²•ë³„ ì¶”ê°€ ì„¤ì •

#### â–· Freeze FT
- BERT ì¸ì½”ë” ì „ì²´ freeze
- classifier(Linear)ë§Œ í•™ìŠµ

#### â–· Full Fine-Tuning
- ëª¨ë“  ë ˆì´ì–´ íŒŒë¼ë¯¸í„° í•™ìŠµ

#### â–· Partial Fine-Tuning
-	BERT ì¸ì½”ë”ì˜ ì¼ë¶€ ë ˆì´ì–´ë§Œ í•™ìŠµí•˜ë„ë¡ ì„¤ì •
-	k = {2, 4} ë‘ ê°€ì§€ ì„¤ì • ëª¨ë‘ ì‹¤í—˜
- ë‘ ì‹¤í—˜ ì¤‘ k = 4ê°€ ë” ë†’ì€ Validation ì„±ëŠ¥ì„ ë³´ì—¬ ìµœì¢… ëª¨ë¸ë¡œ ì„ íƒ

#### â–· BitFit
- ëª¨ë“  ë ˆì´ì–´ì˜ **bias íŒŒë¼ë¯¸í„°**ë§Œ í•™ìŠµ

#### â–· LoRA
- Attentionì˜ Q, V projectionì— ì €ë­í¬ ëª¨ë“ˆ ì ìš©
- rank **r = 4, 8, 16** ê°ê° ì‹¤í—˜
- ê·¸ì¤‘ **r = 8 ëª¨ë¸**ì„ ìµœì¢… LoRA ê²°ê³¼ë¡œ ì‚¬ìš©

---

### 4) í‰ê°€ ì§€í‘œ
ëª¨ë“  ëª¨ë¸ì€ ë™ì¼í•œ test splitìœ¼ë¡œ í‰ê°€í–ˆìœ¼ë©°, ë‹¤ìŒ ì§€í‘œë¥¼ ê³„ì‚°í–ˆìŠµë‹ˆë‹¤.
- Accuracy
- Precision (macro)
- Recall (macro)
- **F1-score (macro)** â†’ ìµœì¢… ë¹„êµ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©

---
## 4. ëª¨ë¸ ì•„í‚¤í…ì²˜ 


![BERT Architecture](https://github.com/user-attachments/assets/932d1c1b-cd05-4020-8731-4baddc865c20)

------------------------------------------------------------
## 5. ì‹¤í—˜ ê²°ê³¼
| Model Name        | Fine-tuning Strategy        | Trainable Params (%)       | F1 Score  |
|-------------------|-----------------------------|-----------------------------|----------|
| Freeze FT         | Head-only (Classifier only) | 0.0014%                     | 0.74  |
| Full Fine-tune     | Full Parameter Training     | 100%                        | 0.96   |
| Partial FT         | Top-4 Layers Only           | 25.897%                     | 0.94   |
| BitFit             | Bias-only Training          | 0.094%                      | 0.93   |
| LoRA              | Low-Rank Adaptation         | 1.209%                   | 0.92 |

![Test F1-scores](https://github.com/user-attachments/assets/c84fe4a4-8f3a-4974-9d02-236a033ddb4f)
## 6. í”„ë¡œì íŠ¸ í´ë” êµ¬ì¡°

```
project/
â”‚
â”œâ”€â”€ models/                     # ëª¨ë¸ ì €ì¥ í´ë”
â”‚   â”œâ”€â”€ freeze_model/           # Head-only fine-tuning ê²°ê³¼ ëª¨ë¸
â”‚   â”œâ”€â”€ full_fine_model/        # Full fine-tuning ê²°ê³¼ ëª¨ë¸
â”‚   â”œâ”€â”€ partial_ft_model/       # Partial (Top-k layers) fine-tuning ëª¨ë¸
â”‚   â”œâ”€â”€ bitfit_model/           # Bias-only (BitFit) fine-tuning ëª¨ë¸
â”‚   â””â”€â”€ lora_model/             # LoRA fine-tuning ëª¨ë¸
â”‚
â”œâ”€â”€ results/                    # í…ŒìŠ¤íŠ¸ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ CSV íŒŒì¼
â”‚   â”œâ”€â”€ freeze_test_outputs.csv
â”‚   â”œâ”€â”€ full_test_outputs.csv
â”‚   â”œâ”€â”€ partial_test_outputs.csv
â”‚   â”œâ”€â”€ bitfit_test_outputs.csv
|   â”œâ”€â”€ lora_test_outputs.csv
â”‚   â””â”€â”€ model_summary.csv
â”‚
â”œâ”€â”€ data/                       # SST-2 ì›ë³¸ ë°ì´í„° ì €ì¥ í´ë”
â”‚   â””â”€â”€ sst2_raw/
â”‚
â”œâ”€â”€ notebooks/                  # ê°œë³„ ì‹¤í—˜ì„ ì‹¤í–‰ Notebook íŒŒì¼
â”‚   â”œâ”€â”€ freeze.ipynb            # Freeze(Head-only) ì‹¤í—˜
â”‚   â”œâ”€â”€ full_fine.ipynb         # Full fine-tuning ì‹¤í—˜
â”‚   â”œâ”€â”€ partial_ft.ipynb        # Partial fine-tuning ì‹¤í—˜
â”‚   â”œâ”€â”€ bitfit.ipynb            # BitFit ì‹¤í—˜
â”‚   â”œâ”€â”€ lora.ipynb              # LoRA ì‹¤í—˜
â”‚   â””â”€â”€ model_test.ipynb        # ëª¨ë“  ëª¨ë¸ì„ ë™ì¼í•œ testsetìœ¼ë¡œ í‰ê°€
â”‚
â”œâ”€â”€ README.md                   
â””â”€â”€ requirements.txt           
```

------------------------------------------------------------

## 7. í´ë” ë° íŒŒì¼ ì„¤ëª…

ğŸ“ **models/**  
â€¢ GitHubì—ëŠ” íŒŒì¼ ìš©ëŸ‰ ë¬¸ì œë¡œ ë¹„ì›Œë‘¡ë‹ˆë‹¤.  
â€¢ Hugging Face Hubì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•´ ì´ í´ë”ì— ë°°ì¹˜í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.  
â€¢ ê° ë””ë ‰í† ë¦¬ëŠ” í•´ë‹¹ íŒŒì¸íŠœë‹ ë°©ì‹ì˜ ëª¨ë¸ì„ ì €ì¥í•˜ê¸° ìœ„í•œ í´ë”ì…ë‹ˆë‹¤.

ğŸ“ **results/**  
â€¢ ê° ëª¨ë¸ì„ ë™ì¼í•œ test splitì—ì„œ í‰ê°€í•œ ê²°ê³¼(csv)ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.  
â€¢ CSV ì»¬ëŸ¼: `sentence, gold, pred`

ğŸ“ **notebooks/**  


Notebook | ì„¤ëª…
-------- | ----
freeze.ipynb | BERT Encoder ë™ê²° + Linear Classifierë§Œ í•™ìŠµ
full_fine.ipynb | BERT ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµ
partial_ft.ipynb | ë§ˆì§€ë§‰ Kê°œì˜ encoder layerë§Œ í•™ìŠµ
bitfit.ipynb | Bias-only íŠœë‹ (BitFit)
lora.ipynb | LoRA íŠœë‹ (r = 4, 8, 16 ë“±)
model_test.ipynb | ì €ì¥ëœ ëª¨ë“  ëª¨ë¸ì„ ë™ì¼í•œ test setì— ëŒ€í•´ í‰ê°€

------------------------------------------------------------

## 8. ì‹¤í–‰ ë°©ë²•

### 1) íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
pip install -r requirements.txt
```
### 2) ê°œë³„ ì‹¤í—˜ ìˆ˜í–‰
ì˜ˆ: Head-only Fine-Tuning â†’ `notebooks/freeze.ipynb`

### 3) ì €ì¥ëœ ëª¨ë¸ í‰ê°€
`notebooks/model_test.ipynb` ì‹¤í–‰

------------------------------------------------------------

## 9. Hugging Face Hub ëª¨ë¸ ë‹¤ìš´ë¡œë“œ


ğŸ“¥ **ì˜ˆì‹œ: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë°©ë²•**


```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(
    "joononeyyy/bert-sst2-freeze"
)
tokenizer = AutoTokenizer.from_pretrained(
    "joononeyyy/bert-sst2-freeze"
)

print("Model and tokenizer loaded successfully!")
```

í•„ìš”í•˜ì‹  ê²½ìš° ëª¨ë¸ ì´ë¦„ë§Œ ë°”ê¿”ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- Freeze FT ëª¨ë¸  
  `joononeyyy/bert-sst2-freeze`

- Full Fine-tuning ëª¨ë¸  
  `joononeyyy/bert-sst2-full`

- Partial Fine-tuning ëª¨ë¸  
  `joononeyyy/bert-sst2-partial`

- BitFit ëª¨ë¸  
  `joononeyyy/bert-sst2-bitfit`

- LoRA ëª¨ë¸  
  `joononeyyy/bert-sst2-lora`





------------------------------------------------------------



