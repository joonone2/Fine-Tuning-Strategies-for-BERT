{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc2c6a51",
   "metadata": {},
   "source": [
    "#### BERT ë³¸ì²´ëŠ” freeze, ë§ˆì§€ë§‰ kê°œ ë ˆì´ì–´ëŠ” freeze í’€ê³ , classifier head ë§Œ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488df6c0",
   "metadata": {},
   "source": [
    "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ & í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84d6a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch, random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"âœ… Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99242cd1",
   "metadata": {},
   "source": [
    "âœ… ë°ì´í„°ì…‹ ë¡œë“œ & 8:1:1 ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e69b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 53879, Val: 6735, Test: 6735\n"
     ]
    }
   ],
   "source": [
    "# SST-2 ì›ë³¸ ë¡œë“œ\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "# ë°ì´í„° ë³€í™˜\n",
    "train_data = train_data.to_pandas()\n",
    "\n",
    "# 8:1:1 ë¶„í• \n",
    "train_split, temp_split = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "val_split, test_split = train_test_split(temp_split, test_size=0.5, random_state=42)\n",
    "\n",
    "# Hugging Face Dataset í˜•íƒœë¡œ ë³€í™˜\n",
    "train_dataset = Dataset.from_dict(train_split)\n",
    "val_dataset   = Dataset.from_dict(val_split)\n",
    "test_dataset  = Dataset.from_dict(test_split)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd848a4",
   "metadata": {},
   "source": [
    "âœ… í† í¬ë‚˜ì´ì € ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6671c0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bert-project/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72bd63c940c046f0b25bbb9d1b85fb76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53879 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410a54572d9a4584b04a3f8e944c3d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdee51af103b4e80b57c64422619d8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess(batch):\n",
    "    return tokenizer(batch[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# ëª¨ë“  splitì— ì ìš©\n",
    "datasets_encoded = {\n",
    "    \"train\": train_dataset.map(preprocess, batched=True),\n",
    "    \"validation\": val_dataset.map(preprocess, batched=True),\n",
    "    \"test\": test_dataset.map(preprocess, batched=True), \n",
    "}\n",
    "\n",
    "# ë°ì´í„°ì…‹ í…ì„œ í˜•ì‹ ì„¤ì •\n",
    "for name in datasets_encoded:\n",
    "    datasets_encoded[name] = datasets_encoded[name].rename_column(\"label\", \"labels\")\n",
    "    datasets_encoded[name].set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e14a5",
   "metadata": {},
   "source": [
    "âœ… ëª¨ë¸ êµ¬ì„± (Encoder Freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8875ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(dropout: float, k_layers: int = 4):\n",
    "    \"\"\"\n",
    "    BERTì˜ ë§ˆì§€ë§‰ kê°œ encoder layer + classifierë§Œ í•™ìŠµí•˜ëŠ” partial fine-tuningìš© ì´ˆê¸°í™” í•¨ìˆ˜.\n",
    "    ex) k_layers=4 â†’ encoder.layer.8~11 + classifier í•™ìŠµ\n",
    "    \"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=2\n",
    "    )\n",
    "\n",
    "    # ğŸ”¹ dropout ì¡°ì • (ìˆìœ¼ë©´)\n",
    "    if hasattr(model, \"dropout\"):\n",
    "        model.dropout.p = dropout\n",
    "\n",
    "    # 1) ì „ì²´ freeze\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # 2) ë§ˆì§€ë§‰ kê°œ encoder layerë§Œ requires_grad=True\n",
    "    total_layers = 12  # BERT-baseëŠ” encoder.layer.0~11\n",
    "    start_layer = total_layers - k_layers\n",
    "\n",
    "    for name, p in model.named_parameters():\n",
    "        # encoder.layer.{start_layer~11}.xxx ì— í•´ë‹¹í•˜ë©´ í•™ìŠµ\n",
    "        for i in range(start_layer, total_layers):\n",
    "            tag = f\"encoder.layer.{i}.\"\n",
    "            if tag in name:\n",
    "                p.requires_grad = True\n",
    "\n",
    "        # classifierëŠ” í•­ìƒ í•™ìŠµ\n",
    "        if name.startswith(\"classifier.\"):\n",
    "            p.requires_grad = True\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # ì²´í¬ìš© ì¶œë ¥\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"ğŸ§© Partial FT (last {k_layers} layers) trainable params: \"\n",
    "          f\"{trainable:,} / {total:,} ({trainable/total*100:.4f}%)\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5036c73",
   "metadata": {},
   "source": [
    "âœ… Optimizer & Loss êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "043c0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizer(model, lr, num_epochs, train_loader):\n",
    "\n",
    "    # ğŸ”¹ requires_grad=True ì¸ íŒŒë¼ë¯¸í„°ë§Œ ì„ íƒ\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(trainable_params, lr=lr)\n",
    "    \n",
    "    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • \n",
    "    num_training_steps = num_epochs * len(train_loader)\n",
    "    scheduler = get_scheduler(\"linear\", optimizer=optimizer,\n",
    "                              num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "    # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    return optimizer, scheduler, criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a22dda5",
   "metadata": {},
   "source": [
    "âœ… í•™ìŠµ ë£¨í”„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aac4bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, train_loader, num_epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch) # forward\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} | Avg Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba27c93d",
   "metadata": {},
   "source": [
    "âœ… í‰ê°€ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6defabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # ğŸ”¹ ì˜ˆì¸¡\n",
    "            logits = model(**batch).logits\n",
    "            pred = logits.argmax(-1)\n",
    "\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    # ğŸ”¹ ì£¼ìš” ì§€í‘œ ê³„ì‚°\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec = precision_score(labels, preds)\n",
    "    rec = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "\n",
    "    print(f\"ğŸ“Š Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}\")\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"prec\": prec,\n",
    "        \"rec\": rec,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b604da",
   "metadata": {},
   "source": [
    "âœ… Random Search ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6bc31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"lr\": [2e-5, 3e-5],\n",
    "    \"batch_size\": [16, 32],\n",
    "    \"dropout\": [0.1, 0.2],\n",
    "    \"epochs\": [3, 4],\n",
    "}\n",
    "\n",
    "n_trials = 5  # ì „ì²´ ì‹¤í—˜ íšŸìˆ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c0428",
   "metadata": {},
   "source": [
    "âœ… Random Search Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f5ed994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "ğŸ”¹ Partial Fine-Tuning: last 2 layers\n",
      "========================================\n",
      "\n",
      "ğŸ¯ [k=2] Trial 1/5 | {'lr': 3e-05, 'batch_size': 16, 'dropout': 0.1, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© Partial FT (last 2 layers) trainable params: 14,177,282 / 109,483,778 (12.9492%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [13:16<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.3183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [12:10<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.2433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [12:09<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.2052\n",
      "ğŸ“Š Acc: 0.9174 | Prec: 0.9333 | Rec: 0.9185 | F1: 0.9258\n",
      "ğŸ“Š Acc: 0.9219 | Prec: 0.9325 | Rec: 0.9281 | F1: 0.9303\n",
      "âœ… [k=2] Val: 91.74% | Test: 92.19%\n",
      "ğŸ’¾ [k=2] Best model updated! (Val: 91.74%)\n",
      "\n",
      "ğŸ¯ [k=2] Trial 2/5 | {'lr': 3e-05, 'batch_size': 32, 'dropout': 0.2, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© Partial FT (last 2 layers) trainable params: 14,177,282 / 109,483,778 (12.9492%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [11:27<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.3266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [11:27<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.2575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [11:28<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.2214\n",
      "ğŸ“Š Acc: 0.9131 | Prec: 0.9354 | Rec: 0.9079 | F1: 0.9214\n",
      "ğŸ“Š Acc: 0.9164 | Prec: 0.9335 | Rec: 0.9165 | F1: 0.9249\n",
      "âœ… [k=2] Val: 91.31% | Test: 91.64%\n",
      "\n",
      "ğŸ¯ [k=2] Trial 3/5 | {'lr': 3e-05, 'batch_size': 32, 'dropout': 0.2, 'epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© Partial FT (last 2 layers) trainable params: 14,177,282 / 109,483,778 (12.9492%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [11:27<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.3239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [11:28<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.2547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [11:26<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.2157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [11:28<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Loss: 0.1917\n",
      "ğŸ“Š Acc: 0.9201 | Prec: 0.9350 | Rec: 0.9217 | F1: 0.9283\n",
      "ğŸ“Š Acc: 0.9274 | Prec: 0.9366 | Rec: 0.9339 | F1: 0.9353\n",
      "âœ… [k=2] Val: 92.01% | Test: 92.74%\n",
      "ğŸ’¾ [k=2] Best model updated! (Val: 92.01%)\n",
      "\n",
      "ğŸ¯ [k=2] Trial 4/5 | {'lr': 3e-05, 'batch_size': 32, 'dropout': 0.2, 'epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© Partial FT (last 2 layers) trainable params: 14,177,282 / 109,483,778 (12.9492%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [11:28<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.3279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [11:27<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.2574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [11:25<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.2187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [11:26<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Loss: 0.1918\n",
      "ğŸ“Š Acc: 0.9183 | Prec: 0.9391 | Rec: 0.9137 | F1: 0.9262\n",
      "ğŸ“Š Acc: 0.9253 | Prec: 0.9383 | Rec: 0.9281 | F1: 0.9332\n",
      "âœ… [k=2] Val: 91.83% | Test: 92.53%\n",
      "\n",
      "ğŸ¯ [k=2] Trial 5/5 | {'lr': 2e-05, 'batch_size': 32, 'dropout': 0.1, 'epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© Partial FT (last 2 layers) trainable params: 14,177,282 / 109,483,778 (12.9492%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [11:28<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.3376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [11:27<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [11:28<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.2460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [11:26<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Loss: 0.2285\n",
      "ğŸ“Š Acc: 0.9079 | Prec: 0.9256 | Rec: 0.9089 | F1: 0.9172\n",
      "ğŸ“Š Acc: 0.9142 | Prec: 0.9254 | Rec: 0.9215 | F1: 0.9234\n",
      "âœ… [k=2] Val: 90.79% | Test: 91.42%\n",
      "\n",
      "========================================\n",
      "ğŸ”¹ Partial Fine-Tuning: last 4 layers\n",
      "========================================\n",
      "\n",
      "ğŸ¯ [k=4] Trial 1/5 | {'lr': 3e-05, 'batch_size': 16, 'dropout': 0.2, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© Partial FT (last 4 layers) trainable params: 28,353,026 / 109,483,778 (25.8970%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [15:01<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.2806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [14:57<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.1924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [14:57<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.1378\n",
      "ğŸ“Š Acc: 0.9416 | Prec: 0.9527 | Rec: 0.9428 | F1: 0.9477\n",
      "ğŸ“Š Acc: 0.9406 | Prec: 0.9469 | Rec: 0.9474 | F1: 0.9471\n",
      "âœ… [k=4] Val: 94.16% | Test: 94.06%\n",
      "ğŸ’¾ [k=4] Best model updated! (Val: 94.16%)\n",
      "\n",
      "ğŸ¯ [k=4] Trial 2/5 | {'lr': 2e-05, 'batch_size': 32, 'dropout': 0.1, 'epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© Partial FT (last 4 layers) trainable params: 28,353,026 / 109,483,778 (25.8970%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [13:55<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.3034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [13:59<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.2238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [13:56<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.1817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [13:56<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Loss: 0.1562\n",
      "ğŸ“Š Acc: 0.9347 | Prec: 0.9472 | Rec: 0.9357 | F1: 0.9414\n",
      "ğŸ“Š Acc: 0.9387 | Prec: 0.9448 | Rec: 0.9461 | F1: 0.9454\n",
      "âœ… [k=4] Val: 93.47% | Test: 93.87%\n",
      "\n",
      "ğŸ¯ [k=4] Trial 3/5 | {'lr': 3e-05, 'batch_size': 16, 'dropout': 0.1, 'epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© Partial FT (last 4 layers) trainable params: 28,353,026 / 109,483,778 (25.8970%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [14:55<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.2855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [14:52<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.1928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [14:56<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.1393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [14:59<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Loss: 0.1114\n",
      "ğŸ“Š Acc: 0.9411 | Prec: 0.9519 | Rec: 0.9426 | F1: 0.9472\n",
      "ğŸ“Š Acc: 0.9425 | Prec: 0.9464 | Rec: 0.9516 | F1: 0.9490\n",
      "âœ… [k=4] Val: 94.11% | Test: 94.25%\n",
      "\n",
      "ğŸ¯ [k=4] Trial 4/5 | {'lr': 2e-05, 'batch_size': 16, 'dropout': 0.1, 'epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© Partial FT (last 4 layers) trainable params: 28,353,026 / 109,483,778 (25.8970%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [14:54<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.2880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [15:02<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.2087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [14:53<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.1570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [14:56<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Loss: 0.1313\n",
      "ğŸ“Š Acc: 0.9415 | Prec: 0.9539 | Rec: 0.9412 | F1: 0.9475\n",
      "ğŸ“Š Acc: 0.9409 | Prec: 0.9476 | Rec: 0.9471 | F1: 0.9474\n",
      "âœ… [k=4] Val: 94.15% | Test: 94.09%\n",
      "\n",
      "ğŸ¯ [k=4] Trial 5/5 | {'lr': 3e-05, 'batch_size': 32, 'dropout': 0.2, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© Partial FT (last 4 layers) trainable params: 28,353,026 / 109,483,778 (25.8970%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [13:56<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.2934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [13:55<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.2099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [13:56<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.1645\n",
      "ğŸ“Š Acc: 0.9351 | Prec: 0.9473 | Rec: 0.9365 | F1: 0.9418\n",
      "ğŸ“Š Acc: 0.9372 | Prec: 0.9421 | Rec: 0.9463 | F1: 0.9442\n",
      "âœ… [k=4] Val: 93.51% | Test: 93.72%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "best_val_acc = 0.0\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "k_candidates = [2, 4]   # ë§ˆì§€ë§‰ ëª‡ ê°œ ë ˆì´ì–´ë¥¼ í’€ì§€\n",
    "\n",
    "for k_layers in k_candidates:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"ğŸ”¹ Partial Fine-Tuning: last {k_layers} layers\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        # ğŸ”¹ ëœë¤ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„ íƒ\n",
    "        params = {k: random.choice(v) for k, v in search_space.items()}\n",
    "        print(f\"\\nğŸ¯ [k={k_layers}] Trial {trial+1}/{n_trials} | {params}\")\n",
    "\n",
    "        # ğŸ”¹ ëª¨ë¸ ì´ˆê¸°í™” (BERT kê°œ ë ˆì´ì–´ + headë§Œ í•™ìŠµ)\n",
    "        model = init_model(params[\"dropout\"], k_layers=k_layers)\n",
    "        train_loader = DataLoader(\n",
    "            datasets_encoded[\"train\"],\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader   = DataLoader(datasets_encoded[\"validation\"], batch_size=64)\n",
    "        test_loader  = DataLoader(datasets_encoded[\"test\"], batch_size=64)\n",
    "\n",
    "        # ğŸ”¹ Optimizer & Scheduler êµ¬ì„±\n",
    "        optimizer, scheduler, criterion = configure_optimizer(\n",
    "            model, params[\"lr\"], params[\"epochs\"], train_loader\n",
    "        )\n",
    "\n",
    "        # ğŸ”¹ í•™ìŠµ ìˆ˜í–‰\n",
    "        train_model(model, optimizer, scheduler, train_loader, num_epochs=params[\"epochs\"])\n",
    "\n",
    "        # ğŸ”¹ Validation / Test í‰ê°€\n",
    "        val_metrics  = evaluate(model, val_loader)\n",
    "        test_metrics = evaluate(model, test_loader)\n",
    "        print(f\"âœ… [k={k_layers}] Val: {val_metrics['acc']*100:.2f}% | \"\n",
    "              f\"Test: {test_metrics['acc']*100:.2f}%\")\n",
    "\n",
    "        # ğŸ”¹ ê²°ê³¼ ì €ì¥\n",
    "        results.append({\n",
    "            \"k_layers\": k_layers,\n",
    "            \"trial\": trial + 1,\n",
    "            \"lr\": params[\"lr\"],\n",
    "            \"batch_size\": params[\"batch_size\"],\n",
    "            \"dropout\": params[\"dropout\"],\n",
    "            \"epochs\": params[\"epochs\"],\n",
    "            \"val_acc\": val_metrics[\"acc\"],\n",
    "            \"val_prec\": val_metrics[\"prec\"],\n",
    "            \"val_rec\": val_metrics[\"rec\"],\n",
    "            \"val_f1\": val_metrics[\"f1\"],\n",
    "            \"test_acc\": test_metrics[\"acc\"],\n",
    "            \"test_prec\": test_metrics[\"prec\"],\n",
    "            \"test_rec\": test_metrics[\"rec\"],\n",
    "            \"test_f1\": test_metrics[\"f1\"]\n",
    "        })\n",
    "\n",
    "        # ğŸ”¹ Best ëª¨ë¸ ì—…ë°ì´íŠ¸ & ì €ì¥ (Val ê¸°ì¤€)\n",
    "        if val_metrics['acc'] > best_val_acc:\n",
    "            best_val_acc = val_metrics['acc']\n",
    "            best_model   = model\n",
    "            best_params  = {\n",
    "                \"k_layers\": k_layers,\n",
    "                **params\n",
    "            }\n",
    "\n",
    "            model.save_pretrained(\"./partial_ft_best_model\")\n",
    "            tokenizer.save_pretrained(\"./partial_ft_best_model\")\n",
    "\n",
    "            print(f\"ğŸ’¾ [k={k_layers}] Best model updated! (Val: {best_val_acc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6518b6",
   "metadata": {},
   "source": [
    "ğŸ“Š ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a842dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"partial_ft_random_search_tvt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff2ca2",
   "metadata": {},
   "source": [
    "ğŸ”¹ ìµœì¢… ê²°ê³¼ ìš”ì•½\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83c93e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Random Search Summary (Sorted by Validation Accuracy):\n",
      "   k_layers  trial       lr  batch_size  dropout  epochs   val_acc  val_prec  \\\n",
      "5         4      1  0.00003          16      0.2       3  0.941648  0.952661   \n",
      "8         4      4  0.00002          16      0.1       4  0.941500  0.953863   \n",
      "7         4      3  0.00003          16      0.1       4  0.941054  0.951885   \n",
      "9         4      5  0.00003          32      0.2       3  0.935115  0.947256   \n",
      "6         4      2  0.00002          32      0.1       4  0.934670  0.947213   \n",
      "2         2      3  0.00003          32      0.2       4  0.920119  0.935016   \n",
      "3         2      4  0.00003          32      0.2       4  0.918337  0.939064   \n",
      "0         2      1  0.00003          16      0.1       3  0.917446  0.933297   \n",
      "1         2      2  0.00003          32      0.2       3  0.913140  0.935370   \n",
      "4         2      5  0.00002          32      0.1       4  0.907944  0.925606   \n",
      "\n",
      "    val_rec    val_f1  test_acc  test_prec  test_rec   test_f1  \n",
      "5  0.942827  0.947719  0.940609   0.946896  0.947396  0.947146  \n",
      "8  0.941239  0.947509  0.940906   0.947633  0.947132  0.947382  \n",
      "7  0.942562  0.947200  0.942539   0.946372  0.951626  0.948992  \n",
      "9  0.936474  0.941834  0.937194   0.942105  0.946339  0.944217  \n",
      "6  0.935680  0.941411  0.938679   0.944826  0.946075  0.945450  \n",
      "2  0.921652  0.928286  0.927394   0.936638  0.933915  0.935275  \n",
      "3  0.913711  0.926214  0.925316   0.938268  0.928099  0.933156  \n",
      "0  0.918475  0.925827  0.921901   0.932537  0.928099  0.930313  \n",
      "1  0.907888  0.921424  0.916407   0.933495  0.916468  0.924903  \n",
      "4  0.908947  0.917201  0.914180   0.925405  0.921491  0.923444  \n",
      "\n",
      "ğŸ† Best Trial Parameters:\n",
      "{'k_layers': 4, 'lr': 3e-05, 'batch_size': 16, 'dropout': 0.2, 'epochs': 3} Val Acc: 94.16%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ“Š Random Search Summary (Sorted by Validation Accuracy):\")\n",
    "print(df.sort_values(\"val_acc\", ascending=False))\n",
    "print(\"\\nğŸ† Best Trial Parameters:\")\n",
    "print(best_params, f\"Val Acc: {best_val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52317617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 28,353,026/109,483,778 (25.897011%)\n"
     ]
    }
   ],
   "source": [
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable:,}/{total:,} ({100*trainable/total:.6f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4d8f855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer params: 28,353,026\n"
     ]
    }
   ],
   "source": [
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(f\"Optimizer params: {sum(p.numel() for p in trainable_params):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17f22286",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(name, param.grad.abs().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "544ce33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f6663",
   "metadata": {},
   "source": [
    "best modle torchë¡œ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed40f400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saved best model weights as partial_ft.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(best_model.state_dict(), \"partial_ft.pt\")\n",
    "print(\"ğŸ’¾ Saved best model weights as partial_ft.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
