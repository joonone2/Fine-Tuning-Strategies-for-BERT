{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc2c6a51",
   "metadata": {},
   "source": [
    "#### BERT ë³¸ì²´, classifier head ëª¨ë‘ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488df6c0",
   "metadata": {},
   "source": [
    "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ & í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84d6a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch, random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"âœ… Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99242cd1",
   "metadata": {},
   "source": [
    "âœ… ë°ì´í„°ì…‹ ë¡œë“œ & 8:1:1 ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e69b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 53879, Val: 6735, Test: 6735\n"
     ]
    }
   ],
   "source": [
    "# SST-2 ì›ë³¸ ë¡œë“œ\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "# ë°ì´í„° ë³€í™˜\n",
    "train_data = train_data.to_pandas()\n",
    "\n",
    "# 8:1:1 ë¶„í• \n",
    "train_split, temp_split = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "val_split, test_split = train_test_split(temp_split, test_size=0.5, random_state=42)\n",
    "\n",
    "# Hugging Face Dataset í˜•íƒœë¡œ ë³€í™˜\n",
    "train_dataset = Dataset.from_dict(train_split)\n",
    "val_dataset   = Dataset.from_dict(val_split)\n",
    "test_dataset  = Dataset.from_dict(test_split)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd848a4",
   "metadata": {},
   "source": [
    "âœ… í† í¬ë‚˜ì´ì € ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6671c0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bert-project/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39947772411a4cd9a0e9471a5cd13250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53879 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e79fc2dff3041c19a8545ba44bb2c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ce28557cc24e2d991e690273df405a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess(batch):\n",
    "    return tokenizer(batch[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# ëª¨ë“  splitì— ì ìš©\n",
    "datasets_encoded = {\n",
    "    \"train\": train_dataset.map(preprocess, batched=True),\n",
    "    \"validation\": val_dataset.map(preprocess, batched=True),\n",
    "    \"test\": test_dataset.map(preprocess, batched=True), \n",
    "}\n",
    "\n",
    "# ë°ì´í„°ì…‹ í…ì„œ í˜•ì‹ ì„¤ì •\n",
    "for name in datasets_encoded:\n",
    "    datasets_encoded[name] = datasets_encoded[name].rename_column(\"label\", \"labels\")\n",
    "    datasets_encoded[name].set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e14a5",
   "metadata": {},
   "source": [
    "âœ… ëª¨ë¸ êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8875ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(dropout: float):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", num_labels=2\n",
    "    ).to(device)\n",
    "\n",
    "    # Dropout ìˆ˜ì •\n",
    "    if hasattr(model, \"dropout\"):\n",
    "        model.dropout.p = dropout\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5036c73",
   "metadata": {},
   "source": [
    "âœ… Optimizer & Loss êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "043c0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizer(model, lr, num_epochs, train_loader):\n",
    "\n",
    "    # classifier.parametersë§Œ ìˆ˜ì •\n",
    "    # ğŸ”¹ requires_grad=True ì¸ íŒŒë¼ë¯¸í„°ë§Œ ì„ íƒ\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(trainable_params, lr=lr)\n",
    "    \n",
    "    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • \n",
    "    num_training_steps = num_epochs * len(train_loader)\n",
    "    scheduler = get_scheduler(\"linear\", optimizer=optimizer,\n",
    "                              num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "    # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    return optimizer, scheduler, criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a22dda5",
   "metadata": {},
   "source": [
    "âœ… í•™ìŠµ ë£¨í”„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aac4bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, train_loader, num_epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch) # forward\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} | Avg Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba27c93d",
   "metadata": {},
   "source": [
    "âœ… í‰ê°€ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6defabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # ğŸ”¹ ì˜ˆì¸¡\n",
    "            logits = model(**batch).logits\n",
    "            pred = logits.argmax(-1)\n",
    "\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    # ğŸ”¹ ì£¼ìš” ì§€í‘œ ê³„ì‚°\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec = precision_score(labels, preds)\n",
    "    rec = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "\n",
    "    print(f\"ğŸ“Š Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}\")\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"prec\": prec,\n",
    "        \"rec\": rec,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b604da",
   "metadata": {},
   "source": [
    "âœ… Random Search ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6bc31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"lr\": [1e-5, 2e-5, 3e-5],\n",
    "    \"batch_size\": [16, 32],\n",
    "    \"dropout\": [0.1, 0.2],\n",
    "    \"epochs\": [3, 4]\n",
    "}\n",
    "\n",
    "n_trials = 5  # ì „ì²´ ì‹¤í—˜ íšŸìˆ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c0428",
   "metadata": {},
   "source": [
    "âœ… Random Search Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f5ed994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ [Trial 1/5] {'lr': 1e-05, 'batch_size': 32, 'dropout': 0.1, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [25:51<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.2368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [25:32<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.1295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [25:30<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.0901\n",
      "ğŸ“Š Acc: 0.9529 | Prec: 0.9604 | Rec: 0.9555 | F1: 0.9579\n",
      "ğŸ“Š Acc: 0.9529 | Prec: 0.9580 | Rec: 0.9582 | F1: 0.9581\n",
      "âœ… Val: 95.29% | Test: 95.29%\n",
      "ğŸ’¾ Best model updated! (Val: 95.29%)\n",
      "\n",
      "ğŸ¯ [Trial 2/5] {'lr': 1e-05, 'batch_size': 32, 'dropout': 0.1, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [26:25<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.2364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [25:33<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.1260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [25:35<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.0878\n",
      "ğŸ“Š Acc: 0.9506 | Prec: 0.9565 | Rec: 0.9553 | F1: 0.9559\n",
      "ğŸ“Š Acc: 0.9523 | Prec: 0.9558 | Rec: 0.9596 | F1: 0.9577\n",
      "âœ… Val: 95.06% | Test: 95.23%\n",
      "\n",
      "ğŸ¯ [Trial 3/5] {'lr': 1e-05, 'batch_size': 16, 'dropout': 0.2, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [28:24<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.2226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [28:14<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.1127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [28:20<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.0719\n",
      "ğŸ“Š Acc: 0.9552 | Prec: 0.9625 | Rec: 0.9574 | F1: 0.9599\n",
      "ğŸ“Š Acc: 0.9514 | Prec: 0.9574 | Rec: 0.9561 | F1: 0.9568\n",
      "âœ… Val: 95.52% | Test: 95.14%\n",
      "ğŸ’¾ Best model updated! (Val: 95.52%)\n",
      "\n",
      "ğŸ¯ [Trial 4/5] {'lr': 3e-05, 'batch_size': 16, 'dropout': 0.1, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [28:18<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.2097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [28:07<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.0934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [28:15<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.0466\n",
      "ğŸ“Š Acc: 0.9553 | Prec: 0.9615 | Rec: 0.9587 | F1: 0.9601\n",
      "ğŸ“Š Acc: 0.9538 | Prec: 0.9583 | Rec: 0.9596 | F1: 0.9589\n",
      "âœ… Val: 95.53% | Test: 95.38%\n",
      "ğŸ’¾ Best model updated! (Val: 95.53%)\n",
      "\n",
      "ğŸ¯ [Trial 5/5] {'lr': 1e-05, 'batch_size': 16, 'dropout': 0.2, 'epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [28:10<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.2326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [28:09<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.1189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [28:08<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.0755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [28:08<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Loss: 0.0506\n",
      "ğŸ“Š Acc: 0.9540 | Prec: 0.9626 | Rec: 0.9550 | F1: 0.9588\n",
      "ğŸ“Š Acc: 0.9543 | Prec: 0.9588 | Rec: 0.9598 | F1: 0.9593\n",
      "âœ… Val: 95.40% | Test: 95.43%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "best_val_acc = 0.0\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    # ğŸ”¹ ëœë¤ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„ íƒ\n",
    "    params = {k: random.choice(v) for k, v in search_space.items()}\n",
    "    print(f\"\\nğŸ¯ [Trial {trial+1}/{n_trials}] {params}\")\n",
    "\n",
    "    # ğŸ”¹ ëª¨ë¸ ì´ˆê¸°í™” (Full - Finetuning)\n",
    "    model = init_model(params[\"dropout\"])\n",
    "    train_loader = DataLoader(datasets_encoded[\"train\"], batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    val_loader   = DataLoader(datasets_encoded[\"validation\"], batch_size=64)\n",
    "    test_loader  = DataLoader(datasets_encoded[\"test\"], batch_size=64)\n",
    "\n",
    "    # ğŸ”¹ Optimizer & Scheduler êµ¬ì„±\n",
    "    optimizer, scheduler, criterion = configure_optimizer(\n",
    "        model, params[\"lr\"], params[\"epochs\"], train_loader\n",
    "    )\n",
    "\n",
    "    # ğŸ”¹ í•™ìŠµ ìˆ˜í–‰\n",
    "    train_model(model, optimizer, scheduler, train_loader, num_epochs=params[\"epochs\"])\n",
    "\n",
    "    # ğŸ”¹ Validation / Test í‰ê°€\n",
    "    val_metrics  = evaluate(model, val_loader)\n",
    "    test_metrics = evaluate(model, test_loader)\n",
    "    print(f\"âœ… Val: {val_metrics['acc']*100:.2f}% | Test: {test_metrics['acc']*100:.2f}%\")\n",
    "\n",
    "    # ğŸ”¹ ê²°ê³¼ ì €ì¥\n",
    "    results.append({\n",
    "        \"trial\": trial + 1,\n",
    "        \"lr\": params[\"lr\"],\n",
    "        \"batch_size\": params[\"batch_size\"],\n",
    "        \"dropout\": params[\"dropout\"],\n",
    "        \"epochs\": params[\"epochs\"],\n",
    "        \"val_acc\": val_metrics[\"acc\"],\n",
    "        \"val_prec\": val_metrics[\"prec\"],\n",
    "        \"val_rec\": val_metrics[\"rec\"],\n",
    "        \"val_f1\": val_metrics[\"f1\"],\n",
    "        \"test_acc\": test_metrics[\"acc\"],\n",
    "        \"test_prec\": test_metrics[\"prec\"],\n",
    "        \"test_rec\": test_metrics[\"rec\"],\n",
    "        \"test_f1\": test_metrics[\"f1\"]\n",
    "    })\n",
    "\n",
    "    # ğŸ”¹ Best ëª¨ë¸ ì—…ë°ì´íŠ¸ & ì €ì¥\n",
    "    if val_metrics['acc'] > best_val_acc:\n",
    "        best_val_acc = val_metrics['acc']\n",
    "        best_model   = model\n",
    "        best_params  = params\n",
    "\n",
    "        model.save_pretrained(\"./full-fine_model\")\n",
    "        tokenizer.save_pretrained(\"./full-fine_model\")\n",
    "\n",
    "        print(f\"ğŸ’¾ Best model updated! (Val: {best_val_acc*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6518b6",
   "metadata": {},
   "source": [
    "ğŸ“Š ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a842dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"full_finetune_random_search_tvt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff2ca2",
   "metadata": {},
   "source": [
    "ğŸ”¹ ìµœì¢… ê²°ê³¼ ìš”ì•½\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83c93e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Random Search Summary (Sorted by Validation Accuracy):\n",
      "   trial       lr  batch_size  dropout  epochs   val_acc  val_prec   val_rec  \\\n",
      "3      4  0.00003          16      0.1       3  0.955308  0.961508  0.958708   \n",
      "2      3  0.00001          16      0.2       3  0.955160  0.962480  0.957385   \n",
      "4      5  0.00001          16      0.2       4  0.953972  0.962647  0.955003   \n",
      "0      1  0.00001          32      0.1       3  0.952932  0.960362  0.955532   \n",
      "1      2  0.00001          32      0.1       3  0.950557  0.956533  0.955267   \n",
      "\n",
      "     val_f1  test_acc  test_prec  test_rec   test_f1  \n",
      "3  0.960106  0.953823   0.958289  0.959556  0.958922  \n",
      "2  0.959926  0.951448   0.957385  0.956119  0.956752  \n",
      "4  0.958809  0.954269   0.958806  0.959820  0.959313  \n",
      "0  0.957941  0.952932   0.957981  0.958234  0.958108  \n",
      "1  0.955900  0.952339   0.955766  0.959556  0.957657  \n",
      "\n",
      "ğŸ† Best Trial Parameters:\n",
      "{'lr': 3e-05, 'batch_size': 16, 'dropout': 0.1, 'epochs': 3} Val Acc: 95.53%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ“Š Random Search Summary (Sorted by Validation Accuracy):\")\n",
    "print(df.sort_values(\"val_acc\", ascending=False))\n",
    "print(\"\\nğŸ† Best Trial Parameters:\")\n",
    "print(best_params, f\"Val Acc: {best_val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52317617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 109,483,778/109,483,778 (100.000000%)\n"
     ]
    }
   ],
   "source": [
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable:,}/{total:,} ({100*trainable/total:.6f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4d8f855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer params: 109,483,778\n"
     ]
    }
   ],
   "source": [
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(f\"Optimizer params: {sum(p.numel() for p in trainable_params):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17f22286",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(name, param.grad.abs().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "544ce33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f6663",
   "metadata": {},
   "source": [
    "best modle torchë¡œ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed40f400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saved best model weights as full-fine_model.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(best_model.state_dict(), \"full-fine_model.pt\")\n",
    "print(\"ğŸ’¾ Saved best model weights as full-fine_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
