{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc2c6a51",
   "metadata": {},
   "source": [
    "#### BERT ë³¸ì²´, classifier headì˜ Biasë§Œ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488df6c0",
   "metadata": {},
   "source": [
    "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ & í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84d6a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch, random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"âœ… Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99242cd1",
   "metadata": {},
   "source": [
    "âœ… ë°ì´í„°ì…‹ ë¡œë“œ & 8:1:1 ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e69b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 53879, Val: 6735, Test: 6735\n"
     ]
    }
   ],
   "source": [
    "# SST-2 ì›ë³¸ ë¡œë“œ\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "# ë°ì´í„° ë³€í™˜\n",
    "train_data = train_data.to_pandas()\n",
    "\n",
    "# 8:1:1 ë¶„í• \n",
    "train_split, temp_split = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "val_split, test_split = train_test_split(temp_split, test_size=0.5, random_state=42)\n",
    "\n",
    "# Hugging Face Dataset í˜•íƒœë¡œ ë³€í™˜\n",
    "train_dataset = Dataset.from_dict(train_split)\n",
    "val_dataset   = Dataset.from_dict(val_split)\n",
    "test_dataset  = Dataset.from_dict(test_split)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd848a4",
   "metadata": {},
   "source": [
    "âœ… í† í¬ë‚˜ì´ì € ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6671c0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bert-project/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d373ec1d9abe499cafe41453e2982a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53879 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf01feaa5d74773857d2964aece68de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1daff6df89ce4505973e87a70e227086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess(batch):\n",
    "    return tokenizer(batch[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# ëª¨ë“  splitì— ì ìš©\n",
    "datasets_encoded = {\n",
    "    \"train\": train_dataset.map(preprocess, batched=True),\n",
    "    \"validation\": val_dataset.map(preprocess, batched=True),\n",
    "    \"test\": test_dataset.map(preprocess, batched=True), \n",
    "}\n",
    "\n",
    "# ë°ì´í„°ì…‹ í…ì„œ í˜•ì‹ ì„¤ì •\n",
    "for name in datasets_encoded:\n",
    "    datasets_encoded[name] = datasets_encoded[name].rename_column(\"label\", \"labels\")\n",
    "    datasets_encoded[name].set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e14a5",
   "metadata": {},
   "source": [
    "âœ… ëª¨ë¸ êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8875ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(dropout: float):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", num_labels=2\n",
    "    ).to(device)\n",
    "\n",
    "    # Dropout ìˆ˜ì •\n",
    "    if hasattr(model, \"dropout\"):\n",
    "        model.dropout.p = dropout\n",
    "\n",
    "    # 1) ì „ë¶€ ì¼ë‹¨ freeze\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # 2) biasë§Œ í’€ê¸° (BitFit í•µì‹¬)\n",
    "    for name, p in model.named_parameters():\n",
    "        if \".bias\" in name:\n",
    "            p.requires_grad = True\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # ì²´í¬ìš© ì¶œë ¥\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"ğŸ§© BitFit trainable params: {trainable:,} / {total:,} \"\n",
    "          f\"({trainable/total*100:.6f}%)\")\n",
    "\n",
    "    # í˜¹ì‹œ ì–´ë–¤ ê²ƒë“¤ì´ ì—´ë ¸ëŠ”ì§€ ë³´ê³  ì‹¶ìœ¼ë©´:\n",
    "    # for n, p in model.named_parameters():\n",
    "    #     if p.requires_grad:\n",
    "    #         print(\"Trainable:\", n)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5036c73",
   "metadata": {},
   "source": [
    "âœ… Optimizer & Loss êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "043c0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizer(model, lr, num_epochs, train_loader):\n",
    "\n",
    "    # classifier.parametersë§Œ ìˆ˜ì •\n",
    "    # ğŸ”¹ requires_grad=True ì¸ íŒŒë¼ë¯¸í„°ë§Œ ì„ íƒ\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(trainable_params, lr=lr)\n",
    "    \n",
    "    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • \n",
    "    num_training_steps = num_epochs * len(train_loader)\n",
    "    scheduler = get_scheduler(\"linear\", optimizer=optimizer,\n",
    "                              num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "    # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    return optimizer, scheduler, criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a22dda5",
   "metadata": {},
   "source": [
    "âœ… í•™ìŠµ ë£¨í”„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aac4bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, train_loader, num_epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch) # forward\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} | Avg Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba27c93d",
   "metadata": {},
   "source": [
    "âœ… í‰ê°€ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6defabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # ğŸ”¹ ì˜ˆì¸¡\n",
    "            logits = model(**batch).logits\n",
    "            pred = logits.argmax(-1)\n",
    "\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    # ğŸ”¹ ì£¼ìš” ì§€í‘œ ê³„ì‚°\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec = precision_score(labels, preds)\n",
    "    rec = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "\n",
    "    print(f\"ğŸ“Š Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}\")\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"prec\": prec,\n",
    "        \"rec\": rec,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b604da",
   "metadata": {},
   "source": [
    "âœ… Random Search ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6bc31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"lr\": [5e-4, 1e-3, 2e-3],\n",
    "    \"batch_size\": [16, 32],\n",
    "    \"dropout\": [0.1, 0.2],\n",
    "    \"epochs\": [3, 4, 5],\n",
    "}\n",
    "\n",
    "n_trials = 5  # ì „ì²´ ì‹¤í—˜ íšŸìˆ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c0428",
   "metadata": {},
   "source": [
    "âœ… Random Search Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5ed994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ [Trial 1/5] {'lr': 0.002, 'batch_size': 16, 'dropout': 0.1, 'epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© BitFit trainable params: 102,914 / 109,483,778 (0.093999%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [23:57<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.3234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [23:51<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.2558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [23:49<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.2222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [23:35<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Loss: 0.1977\n",
      "ğŸ“Š Acc: 0.9253 | Prec: 0.9398 | Rec: 0.9262 | F1: 0.9329\n",
      "ğŸ“Š Acc: 0.9302 | Prec: 0.9430 | Rec: 0.9321 | F1: 0.9375\n",
      "âœ… Val: 92.53% | Test: 93.02%\n",
      "ğŸ’¾ Best model updated! (Val: 92.53%)\n",
      "\n",
      "ğŸ¯ [Trial 2/5] {'lr': 0.002, 'batch_size': 32, 'dropout': 0.2, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© BitFit trainable params: 102,914 / 109,483,778 (0.093999%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [23:16<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.5306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [22:58<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.6383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [22:58<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.6513\n",
      "ğŸ“Š Acc: 0.7286 | Prec: 0.9314 | Rec: 0.5572 | F1: 0.6973\n",
      "ğŸ“Š Acc: 0.7244 | Prec: 0.9250 | Rec: 0.5543 | F1: 0.6932\n",
      "âœ… Val: 72.86% | Test: 72.44%\n",
      "\n",
      "ğŸ¯ [Trial 3/5] {'lr': 0.001, 'batch_size': 32, 'dropout': 0.2, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© BitFit trainable params: 102,914 / 109,483,778 (0.093999%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [23:01<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.4153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [22:58<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.4972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [22:58<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.5429\n",
      "ğŸ“Š Acc: 0.8435 | Prec: 0.8510 | Rec: 0.8740 | F1: 0.8624\n",
      "ğŸ“Š Acc: 0.8439 | Prec: 0.8495 | Rec: 0.8776 | F1: 0.8633\n",
      "âœ… Val: 84.35% | Test: 84.39%\n",
      "\n",
      "ğŸ¯ [Trial 4/5] {'lr': 0.001, 'batch_size': 16, 'dropout': 0.1, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© BitFit trainable params: 102,914 / 109,483,778 (0.093999%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [23:52<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.4276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [23:48<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.6011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [23:47<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.6372\n",
      "ğŸ“Š Acc: 0.7293 | Prec: 0.7030 | Rec: 0.8960 | F1: 0.7879\n",
      "ğŸ“Š Acc: 0.7289 | Prec: 0.7040 | Rec: 0.8927 | F1: 0.7872\n",
      "âœ… Val: 72.93% | Test: 72.89%\n",
      "\n",
      "ğŸ¯ [Trial 5/5] {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1, 'epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© BitFit trainable params: 102,914 / 109,483,778 (0.093999%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [22:56<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.4039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [22:58<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.5224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [22:58<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.5851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [23:00<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Loss: 0.6015\n",
      "ğŸ“Š Acc: 0.8113 | Prec: 0.8677 | Rec: 0.7830 | F1: 0.8232\n",
      "ğŸ“Š Acc: 0.8159 | Prec: 0.8712 | Rec: 0.7888 | F1: 0.8280\n",
      "âœ… Val: 81.13% | Test: 81.59%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "best_val_acc = 0.0\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    # ğŸ”¹ ëœë¤ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„ íƒ\n",
    "    params = {k: random.choice(v) for k, v in search_space.items()}\n",
    "    print(f\"\\nğŸ¯ [Trial {trial+1}/{n_trials}] {params}\")\n",
    "\n",
    "    # ğŸ”¹ ëª¨ë¸ ì´ˆê¸°í™” (Full - Finetuning)\n",
    "    model = init_model(params[\"dropout\"])\n",
    "    train_loader = DataLoader(datasets_encoded[\"train\"], batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    val_loader   = DataLoader(datasets_encoded[\"validation\"], batch_size=64)\n",
    "    test_loader  = DataLoader(datasets_encoded[\"test\"], batch_size=64)\n",
    "\n",
    "    # ğŸ”¹ Optimizer & Scheduler êµ¬ì„±\n",
    "    optimizer, scheduler, criterion = configure_optimizer(\n",
    "        model, params[\"lr\"], params[\"epochs\"], train_loader\n",
    "    )\n",
    "\n",
    "    # ğŸ”¹ í•™ìŠµ ìˆ˜í–‰\n",
    "    train_model(model, optimizer, scheduler, train_loader, num_epochs=params[\"epochs\"])\n",
    "\n",
    "    # ğŸ”¹ Validation / Test í‰ê°€\n",
    "    val_metrics  = evaluate(model, val_loader)\n",
    "    test_metrics = evaluate(model, test_loader)\n",
    "    print(f\"âœ… Val: {val_metrics['acc']*100:.2f}% | Test: {test_metrics['acc']*100:.2f}%\")\n",
    "\n",
    "    # ğŸ”¹ ê²°ê³¼ ì €ì¥\n",
    "    results.append({\n",
    "        \"trial\": trial + 1,\n",
    "        \"lr\": params[\"lr\"],\n",
    "        \"batch_size\": params[\"batch_size\"],\n",
    "        \"dropout\": params[\"dropout\"],\n",
    "        \"epochs\": params[\"epochs\"],\n",
    "        \"val_acc\": val_metrics[\"acc\"],\n",
    "        \"val_prec\": val_metrics[\"prec\"],\n",
    "        \"val_rec\": val_metrics[\"rec\"],\n",
    "        \"val_f1\": val_metrics[\"f1\"],\n",
    "        \"test_acc\": test_metrics[\"acc\"],\n",
    "        \"test_prec\": test_metrics[\"prec\"],\n",
    "        \"test_rec\": test_metrics[\"rec\"],\n",
    "        \"test_f1\": test_metrics[\"f1\"]\n",
    "    })\n",
    "\n",
    "    # ğŸ”¹ Best ëª¨ë¸ ì—…ë°ì´íŠ¸ & ì €ì¥\n",
    "    if val_metrics['acc'] > best_val_acc:\n",
    "        best_val_acc = val_metrics['acc']\n",
    "        best_model   = model \n",
    "        best_params  = params\n",
    "\n",
    "        model.save_pretrained(\"./bitfit_model\")\n",
    "        tokenizer.save_pretrained(\"./bitfit_model\")\n",
    "\n",
    "        print(f\"ğŸ’¾ Best model updated! (Val: {best_val_acc*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6518b6",
   "metadata": {},
   "source": [
    "ğŸ“Š ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a842dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"bitfit_random_search_tvt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff2ca2",
   "metadata": {},
   "source": [
    "ğŸ”¹ ìµœì¢… ê²°ê³¼ ìš”ì•½\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83c93e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Random Search Summary (Sorted by Validation Accuracy):\n",
      "   trial     lr  batch_size  dropout  epochs   val_acc  val_prec   val_rec  \\\n",
      "0      1  0.002          16      0.1       4  0.925316  0.939833  0.926151   \n",
      "2      3  0.001          32      0.2       3  0.843504  0.851031  0.874007   \n",
      "4      5  0.001          32      0.1       4  0.811284  0.867703  0.782954   \n",
      "3      4  0.001          16      0.1       3  0.729324  0.703011  0.895977   \n",
      "1      2  0.002          32      0.2       3  0.728582  0.931416  0.557173   \n",
      "\n",
      "     val_f1  test_acc  test_prec  test_rec   test_f1  \n",
      "0  0.932942  0.930215   0.943033  0.932064  0.937517  \n",
      "2  0.862366  0.843950   0.849539  0.877610  0.863347  \n",
      "4  0.823153  0.815887   0.871241  0.788792  0.827969  \n",
      "3  0.787851  0.728879   0.703982  0.892678  0.787179  \n",
      "1  0.697251  0.724425   0.925011  0.554322  0.693223  \n",
      "\n",
      "ğŸ† Best Trial Parameters:\n",
      "{'lr': 0.002, 'batch_size': 16, 'dropout': 0.1, 'epochs': 4} Val Acc: 92.53%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ“Š Random Search Summary (Sorted by Validation Accuracy):\")\n",
    "print(df.sort_values(\"val_acc\", ascending=False))\n",
    "print(\"\\nğŸ† Best Trial Parameters:\")\n",
    "print(best_params, f\"Val Acc: {best_val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52317617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 102,914/109,483,778 (0.093999%)\n"
     ]
    }
   ],
   "source": [
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable:,}/{total:,} ({100*trainable/total:.6f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4d8f855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer params: 102,914\n"
     ]
    }
   ],
   "source": [
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(f\"Optimizer params: {sum(p.numel() for p in trainable_params):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17f22286",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(name, param.grad.abs().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "544ce33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f6663",
   "metadata": {},
   "source": [
    "best modle torchë¡œ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed40f400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saved best model weights as bitfit_model.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(best_model.state_dict(), \"bitfit_model.pt\")\n",
    "print(\"ğŸ’¾ Saved best model weights as bitfit_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
