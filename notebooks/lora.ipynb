{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc2c6a51",
   "metadata": {},
   "source": [
    "#### Backbone(BERT)ÏùÄ freeze, classifier + LoRA Î™®ÎìàÎßå ÌïôÏäµ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488df6c0",
   "metadata": {},
   "source": [
    "‚úÖ ÎùºÏù¥Î∏åÎü¨Î¶¨ & ÌôòÍ≤Ω ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84d6a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch, random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"‚úÖ Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99242cd1",
   "metadata": {},
   "source": [
    "‚úÖ Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú & 8:1:1 Î∂ÑÌï†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e69b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 53879, Val: 6735, Test: 6735\n"
     ]
    }
   ],
   "source": [
    "# SST-2 ÏõêÎ≥∏ Î°úÎìú\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò\n",
    "train_data = train_data.to_pandas()\n",
    "\n",
    "# 8:1:1 Î∂ÑÌï†\n",
    "train_split, temp_split = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "val_split, test_split = train_test_split(temp_split, test_size=0.5, random_state=42)\n",
    "\n",
    "# Hugging Face Dataset ÌòïÌÉúÎ°ú Î≥ÄÌôò\n",
    "train_dataset = Dataset.from_dict(train_split)\n",
    "val_dataset   = Dataset.from_dict(val_split)\n",
    "test_dataset  = Dataset.from_dict(test_split)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd848a4",
   "metadata": {},
   "source": [
    "‚úÖ ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î∞è Ï†ÑÏ≤òÎ¶¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6671c0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bert-project/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33be05888efd4bc0b91780d65d014513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53879 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00753a324bd1453783adf651f684b3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cabe6c19b634b3e968494de113ca645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò\n",
    "def preprocess(batch):\n",
    "    return tokenizer(batch[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Î™®Îì† splitÏóê Ï†ÅÏö©\n",
    "datasets_encoded = {\n",
    "    \"train\": train_dataset.map(preprocess, batched=True),\n",
    "    \"validation\": val_dataset.map(preprocess, batched=True),\n",
    "    \"test\": test_dataset.map(preprocess, batched=True), \n",
    "}\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã ÌÖêÏÑú ÌòïÏãù ÏÑ§Ï†ï\n",
    "for name in datasets_encoded:\n",
    "    datasets_encoded[name] = datasets_encoded[name].rename_column(\"label\", \"labels\")\n",
    "    datasets_encoded[name].set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e14a5",
   "metadata": {},
   "source": [
    "‚úÖ Î™®Îç∏ Íµ¨ÏÑ± (Encoder Freeze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7486b88",
   "metadata": {},
   "source": [
    "Query, ValueÏóêÎßå LoraÏ†ÅÏö©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8875ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(dropout, r=8, alpha=16, target_modules=[\"query\", \"value\"]):\n",
    "    # 1) Î≤†Ïù¥Ïä§ Î™®Îç∏ Î°úÎìú\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=2\n",
    "    )\n",
    "\n",
    "    # 2) dropout Î∞òÏòÅ\n",
    "    if hasattr(model, \"dropout\"):\n",
    "        model.dropout.p = dropout\n",
    "\n",
    "    # 3) Ï†ÑÏ≤¥ freeze (BERT backbone)\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # 4) classifierÎäî Ìï≠ÏÉÅ ÌïôÏäµ\n",
    "    for name, p in model.named_parameters():\n",
    "        if name.startswith(\"classifier.\"):\n",
    "            p.requires_grad = True\n",
    "\n",
    "    # 5) LoRA ÏÑ§Ï†ï\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        target_modules=target_modules,   # attentionÏùò Q,V\n",
    "        lora_dropout=0.05,\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        bias=\"none\"\n",
    "    )\n",
    "\n",
    "    # 6) LoRA Ï†ÅÏö©\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5036c73",
   "metadata": {},
   "source": [
    "‚úÖ Optimizer & Loss Íµ¨ÏÑ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "043c0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizer(model, lr, num_epochs, train_loader):\n",
    "\n",
    "    # classifier.parametersÎßå ÏàòÏ†ï\n",
    "    # üîπ requires_grad=True Ïù∏ ÌååÎùºÎØ∏ÌÑ∞Îßå ÏÑ†ÌÉù\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(trainable_params, lr=lr)\n",
    "    \n",
    "    # ÌïôÏäµÎ•† Ïä§ÏºÄÏ§ÑÎü¨ ÏÑ§Ï†ï \n",
    "    num_training_steps = num_epochs * len(train_loader)\n",
    "    scheduler = get_scheduler(\"linear\", optimizer=optimizer,\n",
    "                              num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "    # ÏÜêÏã§ Ìï®Ïàò ÏÑ§Ï†ï\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    return optimizer, scheduler, criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a22dda5",
   "metadata": {},
   "source": [
    "‚úÖ ÌïôÏäµ Î£®ÌîÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aac4bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, train_loader, num_epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch) # forward\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} | Avg Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba27c93d",
   "metadata": {},
   "source": [
    "‚úÖ ÌèâÍ∞Ä Ìï®Ïàò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6defabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # üîπ ÏòàÏ∏°\n",
    "            logits = model(**batch).logits\n",
    "            pred = logits.argmax(-1)\n",
    "\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    # üîπ Ï£ºÏöî ÏßÄÌëú Í≥ÑÏÇ∞\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec = precision_score(labels, preds)\n",
    "    rec = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "\n",
    "    print(f\"üìä Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}\")\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"prec\": prec,\n",
    "        \"rec\": rec,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b604da",
   "metadata": {},
   "source": [
    "‚úÖ Random Search ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6bc31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ïã§ÌóòÌï† LoRA rank Í∞íÎì§\n",
    "r_list = [4, 8, 16]\n",
    "\n",
    "# ÎûúÎç§ ÏÑúÏπòÏö© ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í≥µÍ∞Ñ\n",
    "search_space = {\n",
    "    \"lr\":        [2e-5, 3e-5, 5e-5],\n",
    "    \"batch_size\":[16, 32],\n",
    "    \"dropout\":   [0.1, 0.2],\n",
    "    \"epochs\":    [2, 3, 4],\n",
    "}\n",
    "\n",
    "n_trials_per_r = 3   # rÎßàÎã§ Î™á Î≤àÏî© ÎèåÎ†§Î≥ºÏßÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c0428",
   "metadata": {},
   "source": [
    "‚úÖ Random Search Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f5ed994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üî∑ Running experiments for LoRA rank r = 4\n",
      "==================================================\n",
      "\n",
      "üéØ [r=4] Trial 1/3 | Params = {'lr': 3e-05, 'batch_size': 32, 'dropout': 0.2, 'epochs': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [18:28<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.3965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [18:32<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.2898\n",
      "üìä Acc: 0.8851 | Prec: 0.9055 | Rec: 0.8878 | F1: 0.8966\n",
      "üìä Acc: 0.8944 | Prec: 0.9118 | Rec: 0.8990 | F1: 0.9054\n",
      "üìå [r=4] Val Acc: 88.51% | Test Acc: 89.44%\n",
      "üíæ Best model updated! r=4, Val=88.51% (Saved ‚Üí ./lora_best_r4)\n",
      "\n",
      "üéØ [r=4] Trial 2/3 | Params = {'lr': 3e-05, 'batch_size': 16, 'dropout': 0.2, 'epochs': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3368/3368 [19:33<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.3630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3368/3368 [19:32<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.2798\n",
      "üìä Acc: 0.8878 | Prec: 0.9071 | Rec: 0.8912 | F1: 0.8991\n",
      "üìä Acc: 0.8959 | Prec: 0.9112 | Rec: 0.9027 | F1: 0.9069\n",
      "üìå [r=4] Val Acc: 88.78% | Test Acc: 89.59%\n",
      "üíæ Best model updated! r=4, Val=88.78% (Saved ‚Üí ./lora_best_r4)\n",
      "\n",
      "üéØ [r=4] Trial 3/3 | Params = {'lr': 5e-05, 'batch_size': 32, 'dropout': 0.1, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [18:54<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.3480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [18:53<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.2620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [19:10<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.2509\n",
      "üìä Acc: 0.8987 | Prec: 0.9182 | Rec: 0.8997 | F1: 0.9088\n",
      "üìä Acc: 0.9073 | Prec: 0.9224 | Rec: 0.9117 | F1: 0.9170\n",
      "üìå [r=4] Val Acc: 89.87% | Test Acc: 90.73%\n",
      "üíæ Best model updated! r=4, Val=89.87% (Saved ‚Üí ./lora_best_r4)\n",
      "\n",
      "==================================================\n",
      "üî∑ Running experiments for LoRA rank r = 8\n",
      "==================================================\n",
      "\n",
      "üéØ [r=8] Trial 1/3 | Params = {'lr': 3e-05, 'batch_size': 32, 'dropout': 0.2, 'epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [19:28<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.3855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [19:36<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.2767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [18:49<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.2617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [19:22<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Loss: 0.2570\n",
      "üìä Acc: 0.8924 | Prec: 0.9076 | Rec: 0.8997 | F1: 0.9036\n",
      "üìä Acc: 0.9036 | Prec: 0.9137 | Rec: 0.9149 | F1: 0.9143\n",
      "üìå [r=8] Val Acc: 89.24% | Test Acc: 90.36%\n",
      "\n",
      "üéØ [r=8] Trial 2/3 | Params = {'lr': 5e-05, 'batch_size': 32, 'dropout': 0.1, 'epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [19:56<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [18:56<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.2605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [18:56<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.2448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [18:56<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Loss: 0.2351\n",
      "üìä Acc: 0.9032 | Prec: 0.9213 | Rec: 0.9047 | F1: 0.9129\n",
      "üìä Acc: 0.9131 | Prec: 0.9271 | Rec: 0.9175 | F1: 0.9223\n",
      "üìå [r=8] Val Acc: 90.32% | Test Acc: 91.31%\n",
      "üíæ Best model updated! r=8, Val=90.32% (Saved ‚Üí ./lora_best_r8)\n",
      "\n",
      "üéØ [r=8] Trial 3/3 | Params = {'lr': 3e-05, 'batch_size': 32, 'dropout': 0.1, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [18:52<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.3803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [19:03<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.2792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [18:53<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.2676\n",
      "üìä Acc: 0.8925 | Prec: 0.9085 | Rec: 0.8989 | F1: 0.9037\n",
      "üìä Acc: 0.8983 | Prec: 0.9089 | Rec: 0.9101 | F1: 0.9095\n",
      "üìå [r=8] Val Acc: 89.25% | Test Acc: 89.83%\n",
      "\n",
      "==================================================\n",
      "üî∑ Running experiments for LoRA rank r = 16\n",
      "==================================================\n",
      "\n",
      "üéØ [r=16] Trial 1/3 | Params = {'lr': 3e-05, 'batch_size': 32, 'dropout': 0.2, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [18:23<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.3921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [18:24<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.2822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [18:24<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.2682\n",
      "üìä Acc: 0.8897 | Prec: 0.9061 | Rec: 0.8962 | F1: 0.9011\n",
      "üìä Acc: 0.8987 | Prec: 0.9094 | Rec: 0.9104 | F1: 0.9099\n",
      "üìå [r=16] Val Acc: 88.97% | Test Acc: 89.87%\n",
      "\n",
      "üéØ [r=16] Trial 2/3 | Params = {'lr': 5e-05, 'batch_size': 16, 'dropout': 0.2, 'epochs': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3368/3368 [19:11<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.3238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3368/3368 [19:09<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.2559\n",
      "üìä Acc: 0.8959 | Prec: 0.9102 | Rec: 0.9037 | F1: 0.9069\n",
      "üìä Acc: 0.9041 | Prec: 0.9133 | Rec: 0.9162 | F1: 0.9148\n",
      "üìå [r=16] Val Acc: 89.59% | Test Acc: 90.41%\n",
      "\n",
      "üéØ [r=16] Trial 3/3 | Params = {'lr': 2e-05, 'batch_size': 32, 'dropout': 0.2, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [18:22<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.4305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [18:21<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.2978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1684/1684 [18:24<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.2827\n",
      "üìä Acc: 0.8857 | Prec: 0.9021 | Rec: 0.8931 | F1: 0.8976\n",
      "üìä Acc: 0.8941 | Prec: 0.9029 | Rec: 0.9093 | F1: 0.9061\n",
      "üìå [r=16] Val Acc: 88.57% | Test Acc: 89.41%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "best_val_acc = 0.0\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "for r in r_list:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"üî∑ Running experiments for LoRA rank r = {r}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for trial in range(n_trials_per_r):\n",
    "        # üîπ ÎûúÎç§ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏÑ†ÌÉù\n",
    "        params = {k: random.choice(v) for k, v in search_space.items()}\n",
    "        print(f\"\\nüéØ [r={r}] Trial {trial+1}/{n_trials_per_r} | Params = {params}\")\n",
    "\n",
    "        # üîπ Î™®Îç∏ Ï¥àÍ∏∞Ìôî (LoRA Ï†ÅÏö©)\n",
    "        model = init_model(params[\"dropout\"], r=r)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            datasets_encoded[\"train\"], \n",
    "            batch_size=params[\"batch_size\"], \n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader  = DataLoader(datasets_encoded[\"validation\"], batch_size=64)\n",
    "        test_loader = DataLoader(datasets_encoded[\"test\"], batch_size=64)\n",
    "\n",
    "        # üîπ Optimizer & Scheduler Íµ¨ÏÑ±\n",
    "        optimizer, scheduler, criterion = configure_optimizer(\n",
    "            model, params[\"lr\"], params[\"epochs\"], train_loader\n",
    "        )\n",
    "\n",
    "        # üîπ ÌïôÏäµ\n",
    "        train_model(model, optimizer, scheduler, train_loader, num_epochs=params[\"epochs\"])\n",
    "\n",
    "        # üîπ ÏÑ±Îä• ÌèâÍ∞Ä\n",
    "        val_metrics  = evaluate(model, val_loader)\n",
    "        test_metrics = evaluate(model, test_loader)\n",
    "\n",
    "        print(f\"üìå [r={r}] Val Acc: {val_metrics['acc']*100:.2f}% | Test Acc: {test_metrics['acc']*100:.2f}%\")\n",
    "\n",
    "        # üîπ Í≤∞Í≥º Ï†ÄÏû•\n",
    "        results.append({\n",
    "            \"r\": r,\n",
    "            \"trial\": trial + 1,\n",
    "            \"lr\": params[\"lr\"],\n",
    "            \"batch_size\": params[\"batch_size\"],\n",
    "            \"dropout\": params[\"dropout\"],\n",
    "            \"epochs\": params[\"epochs\"],\n",
    "            \"val_acc\": val_metrics[\"acc\"],\n",
    "            \"val_prec\": val_metrics[\"prec\"],\n",
    "            \"val_rec\": val_metrics[\"rec\"],\n",
    "            \"val_f1\": val_metrics[\"f1\"],\n",
    "            \"test_acc\": test_metrics[\"acc\"],\n",
    "            \"test_prec\": test_metrics[\"prec\"],\n",
    "            \"test_rec\": test_metrics[\"rec\"],\n",
    "            \"test_f1\": test_metrics[\"f1\"],\n",
    "        })\n",
    "\n",
    "        # üîπ Î≤†Ïä§Ìä∏ Î™®Îç∏ Ï†ÄÏû•\n",
    "        if val_metrics[\"acc\"] > best_val_acc:\n",
    "            best_val_acc = val_metrics[\"acc\"]\n",
    "            best_model = model\n",
    "            best_params = {**params, \"r\": r}\n",
    "\n",
    "            save_dir = f\"./lora_best_r{r}\"\n",
    "            model.save_pretrained(save_dir)\n",
    "            tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "            print(f\"üíæ Best model updated! r={r}, Val={best_val_acc*100:.2f}% (Saved ‚Üí {save_dir})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6518b6",
   "metadata": {},
   "source": [
    "üìä Í≤∞Í≥º Ï†ïÎ¶¨ Î∞è Ï†ÄÏû•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff2ca2",
   "metadata": {},
   "source": [
    "üîπ ÏµúÏ¢Ö Í≤∞Í≥º ÏöîÏïΩ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83c93e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Random Search Summary (Sorted by Validation Accuracy):\n",
      "    r  trial       lr  batch_size  dropout  epochs   val_acc  val_prec  \\\n",
      "4   8      2  0.00005          32      0.1       4  0.903192  0.921294   \n",
      "2   4      3  0.00005          32      0.1       3  0.898738  0.918152   \n",
      "7  16      2  0.00005          16      0.2       2  0.895917  0.910157   \n",
      "5   8      3  0.00003          32      0.1       3  0.892502  0.908507   \n",
      "3   8      1  0.00003          32      0.2       4  0.892353  0.907610   \n",
      "6  16      1  0.00003          32      0.2       3  0.889681  0.906074   \n",
      "1   4      2  0.00003          16      0.2       2  0.887751  0.907058   \n",
      "8  16      3  0.00002          32      0.2       3  0.885672  0.902139   \n",
      "0   4      1  0.00003          32      0.2       2  0.885078  0.905508   \n",
      "\n",
      "    val_rec    val_f1  test_acc  test_prec  test_rec   test_f1  \n",
      "4  0.904711  0.912927  0.913140   0.927083  0.917526  0.922280  \n",
      "2  0.899682  0.908824  0.907350   0.922439  0.911710  0.917043  \n",
      "7  0.903653  0.906893  0.904083   0.913307  0.916204  0.914753  \n",
      "5  0.898888  0.903672  0.898293   0.908923  0.910124  0.909523  \n",
      "3  0.899682  0.903629  0.903638   0.913675  0.914882  0.914278  \n",
      "6  0.896241  0.901131  0.898738   0.909427  0.910389  0.909908  \n",
      "1  0.891212  0.899065  0.895917   0.911153  0.902723  0.906918  \n",
      "8  0.893065  0.897579  0.894135   0.902887  0.909331  0.906098  \n",
      "0  0.887771  0.896552  0.894432   0.911796  0.899022  0.905364  \n",
      "\n",
      "üèÜ Best Trial Parameters:\n",
      "{'lr': 5e-05, 'batch_size': 32, 'dropout': 0.1, 'epochs': 4, 'r': 8} Val Acc: 90.32%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìä Random Search Summary (Sorted by Validation Accuracy):\")\n",
    "print(df.sort_values(\"val_acc\", ascending=False))\n",
    "print(\"\\nüèÜ Best Trial Parameters:\")\n",
    "print(best_params, f\"Val Acc: {best_val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bb0bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"lora_finetune_random_search_tvt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52317617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 591,362/110,075,140 (0.537235%)\n"
     ]
    }
   ],
   "source": [
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable:,}/{total:,} ({100*trainable/total:.6f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4d8f855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer params: 591,362\n"
     ]
    }
   ],
   "source": [
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(f\"Optimizer params: {sum(p.numel() for p in trainable_params):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17f22286",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(name, param.grad.abs().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "544ce33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): BertForSequenceClassification(\n",
      "      (bert): BertModel(\n",
      "        (embeddings): BertEmbeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (token_type_embeddings): Embedding(2, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): BertEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSdpaSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): BertPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f6663",
   "metadata": {},
   "source": [
    "best modle torchÎ°ú Ï†ÄÏû•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed40f400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved best model weights as lora_model.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(best_model.state_dict(), \"lora_model.pt\")\n",
    "print(\"üíæ Saved best model weights as lora_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
