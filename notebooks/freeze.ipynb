{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc2c6a51",
   "metadata": {},
   "source": [
    "#### BERT ë³¸ì²´ëŠ” ëª¨ë‘ freezeì‹œí‚¤ê³ , classifier head ë§Œ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488df6c0",
   "metadata": {},
   "source": [
    "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ & í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84d6a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch, random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"âœ… Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99242cd1",
   "metadata": {},
   "source": [
    "âœ… ë°ì´í„°ì…‹ ë¡œë“œ & 8:1:1 ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e69b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 53879, Val: 6735, Test: 6735\n"
     ]
    }
   ],
   "source": [
    "# SST-2 ì›ë³¸ ë¡œë“œ\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "# ë°ì´í„° ë³€í™˜\n",
    "train_data = train_data.to_pandas()\n",
    "\n",
    "# 8:1:1 ë¶„í• \n",
    "train_split, temp_split = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "val_split, test_split = train_test_split(temp_split, test_size=0.5, random_state=42)\n",
    "\n",
    "# Hugging Face Dataset í˜•íƒœë¡œ ë³€í™˜\n",
    "train_dataset = Dataset.from_dict(train_split)\n",
    "val_dataset   = Dataset.from_dict(val_split)\n",
    "test_dataset  = Dataset.from_dict(test_split)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd848a4",
   "metadata": {},
   "source": [
    "âœ… í† í¬ë‚˜ì´ì € ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6671c0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bert-project/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9354c2f5257846a6aa38443a9aee113c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53879 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5bb8b35965479fbfe51bf70b3bff94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64db127c9544e31986cf0b4593c7fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess(batch):\n",
    "    return tokenizer(batch[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# ëª¨ë“  splitì— ì ìš©\n",
    "datasets_encoded = {\n",
    "    \"train\": train_dataset.map(preprocess, batched=True),\n",
    "    \"validation\": val_dataset.map(preprocess, batched=True),\n",
    "    \"test\": test_dataset.map(preprocess, batched=True), \n",
    "}\n",
    "\n",
    "# ë°ì´í„°ì…‹ í…ì„œ í˜•ì‹ ì„¤ì •\n",
    "for name in datasets_encoded:\n",
    "    datasets_encoded[name] = datasets_encoded[name].rename_column(\"label\", \"labels\")\n",
    "    datasets_encoded[name].set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e14a5",
   "metadata": {},
   "source": [
    "âœ… ëª¨ë¸ êµ¬ì„± (Encoder Freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8875ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(dropout: float):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", num_labels=2\n",
    "    ).to(device)\n",
    "\n",
    "    # Dropout ìˆ˜ì • (headì—ë§Œ ë°˜ì˜)\n",
    "    if hasattr(model, \"dropout\"):\n",
    "        model.dropout.p = dropout\n",
    "\n",
    "    # Encoder(BERT ë³¸ì²´) freeze\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Classifier headë§Œ í•™ìŠµ\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5036c73",
   "metadata": {},
   "source": [
    "âœ… Optimizer & Loss êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "043c0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizer(model, lr, num_epochs, train_loader):\n",
    "\n",
    "    # classifier.parametersë§Œ ìˆ˜ì •\n",
    "    # ğŸ”¹ requires_grad=True ì¸ íŒŒë¼ë¯¸í„°ë§Œ ì„ íƒ\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(trainable_params, lr=lr)\n",
    "    \n",
    "    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • \n",
    "    num_training_steps = num_epochs * len(train_loader)\n",
    "    scheduler = get_scheduler(\"linear\", optimizer=optimizer,\n",
    "                              num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "    # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    return optimizer, scheduler, criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a22dda5",
   "metadata": {},
   "source": [
    "âœ… í•™ìŠµ ë£¨í”„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aac4bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, train_loader, num_epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch) # forward\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} | Avg Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba27c93d",
   "metadata": {},
   "source": [
    "âœ… í‰ê°€ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6defabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # ğŸ”¹ ì˜ˆì¸¡\n",
    "            logits = model(**batch).logits\n",
    "            pred = logits.argmax(-1)\n",
    "\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    # ğŸ”¹ ì£¼ìš” ì§€í‘œ ê³„ì‚°\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec = precision_score(labels, preds)\n",
    "    rec = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "\n",
    "    print(f\"ğŸ“Š Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}\")\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"prec\": prec,\n",
    "        \"rec\": rec,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b604da",
   "metadata": {},
   "source": [
    "âœ… Random Search ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6bc31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"lr\": [1e-5, 2e-5, 3e-5, 5e-5, 1e-4],   # í•™ìŠµë¥ \n",
    "    \"batch_size\": [16, 32, 64],             # ë°°ì¹˜ í¬ê¸°\n",
    "    \"dropout\": [0.1, 0.2, 0.3],             # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨\n",
    "    \"epochs\": [2, 3, 4]                     # í•™ìŠµ epoch ìˆ˜\n",
    "}\n",
    "\n",
    "n_trials = 10  # ì „ì²´ ì‹¤í—˜ íšŸìˆ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c0428",
   "metadata": {},
   "source": [
    "âœ… Random Search Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f5ed994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ [Trial 1/10] {'lr': 0.0001, 'batch_size': 64, 'dropout': 0.3, 'epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 842/842 [08:41<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.6730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 842/842 [09:50<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.6394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 842/842 [10:45<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.6228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 842/842 [08:40<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Loss: 0.6152\n",
      "ğŸ“Š Acc: 0.6970 | Prec: 0.6683 | Rec: 0.9127 | F1: 0.7716\n",
      "ğŸ“Š Acc: 0.6986 | Prec: 0.6686 | Rec: 0.9188 | F1: 0.7740\n",
      "âœ… Val: 69.70% | Test: 69.86%\n",
      "ğŸ’¾ Best model updated! (Val: 69.70%)\n",
      "\n",
      "ğŸ¯ [Trial 2/10] {'lr': 5e-05, 'batch_size': 64, 'dropout': 0.1, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 842/842 [08:42<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.6728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 842/842 [08:42<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.6521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 842/842 [08:41<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.6439\n",
      "ğŸ“Š Acc: 0.6386 | Prec: 0.6148 | Rec: 0.9529 | F1: 0.7474\n",
      "ğŸ“Š Acc: 0.6370 | Prec: 0.6137 | Rec: 0.9545 | F1: 0.7471\n",
      "âœ… Val: 63.86% | Test: 63.70%\n",
      "\n",
      "ğŸ¯ [Trial 3/10] {'lr': 2e-05, 'batch_size': 32, 'dropout': 0.1, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [08:56<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.6864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [08:57<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.6705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [08:59<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.6642\n",
      "ğŸ“Š Acc: 0.5930 | Prec: 0.5816 | Rec: 0.9780 | F1: 0.7294\n",
      "ğŸ“Š Acc: 0.5985 | Prec: 0.5850 | Rec: 0.9812 | F1: 0.7330\n",
      "âœ… Val: 59.30% | Test: 59.85%\n",
      "\n",
      "ğŸ¯ [Trial 4/10] {'lr': 1e-05, 'batch_size': 16, 'dropout': 0.3, 'epochs': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [09:28<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.6964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [09:30<00:00,  5.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.6901\n",
      "ğŸ“Š Acc: 0.5604 | Prec: 0.5619 | Rec: 0.9815 | F1: 0.7147\n",
      "ğŸ“Š Acc: 0.5709 | Prec: 0.5676 | Rec: 0.9915 | F1: 0.7219\n",
      "âœ… Val: 56.04% | Test: 57.09%\n",
      "\n",
      "ğŸ¯ [Trial 5/10] {'lr': 3e-05, 'batch_size': 16, 'dropout': 0.3, 'epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [09:27<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.6810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [09:29<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.6564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [09:28<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.6430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [09:28<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Loss: 0.6367\n",
      "ğŸ“Š Acc: 0.6561 | Prec: 0.6305 | Rec: 0.9346 | F1: 0.7530\n",
      "ğŸ“Š Acc: 0.6579 | Prec: 0.6310 | Rec: 0.9418 | F1: 0.7557\n",
      "âœ… Val: 65.61% | Test: 65.79%\n",
      "\n",
      "ğŸ¯ [Trial 6/10] {'lr': 0.0001, 'batch_size': 64, 'dropout': 0.2, 'epochs': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 842/842 [08:45<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.6689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 842/842 [08:42<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.6429\n",
      "ğŸ“Š Acc: 0.6517 | Prec: 0.6243 | Rec: 0.9516 | F1: 0.7540\n",
      "ğŸ“Š Acc: 0.6561 | Prec: 0.6276 | Rec: 0.9537 | F1: 0.7570\n",
      "âœ… Val: 65.17% | Test: 65.61%\n",
      "\n",
      "ğŸ¯ [Trial 7/10] {'lr': 0.0001, 'batch_size': 32, 'dropout': 0.3, 'epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [09:00<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.6623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [08:56<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.6217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [08:58<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.6044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [08:59<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Loss: 0.5982\n",
      "ğŸ“Š Acc: 0.7330 | Prec: 0.7111 | Rec: 0.8827 | F1: 0.7877\n",
      "ğŸ“Š Acc: 0.7408 | Prec: 0.7159 | Rec: 0.8927 | F1: 0.7946\n",
      "âœ… Val: 73.30% | Test: 74.08%\n",
      "ğŸ’¾ Best model updated! (Val: 73.30%)\n",
      "\n",
      "ğŸ¯ [Trial 8/10] {'lr': 0.0001, 'batch_size': 16, 'dropout': 0.3, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [09:28<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.6524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [09:26<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.6094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3368/3368 [09:29<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.5965\n",
      "ğŸ“Š Acc: 0.7258 | Prec: 0.6994 | Rec: 0.8962 | F1: 0.7857\n",
      "ğŸ“Š Acc: 0.7314 | Prec: 0.7025 | Rec: 0.9051 | F1: 0.7910\n",
      "âœ… Val: 72.58% | Test: 73.14%\n",
      "\n",
      "ğŸ¯ [Trial 9/10] {'lr': 3e-05, 'batch_size': 64, 'dropout': 0.3, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 842/842 [08:41<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.6910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 842/842 [08:43<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.6767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 842/842 [08:39<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 0.6713\n",
      "ğŸ“Š Acc: 0.5887 | Prec: 0.5786 | Rec: 0.9825 | F1: 0.7283\n",
      "ğŸ“Š Acc: 0.5871 | Prec: 0.5778 | Rec: 0.9831 | F1: 0.7279\n",
      "âœ… Val: 58.87% | Test: 58.71%\n",
      "\n",
      "ğŸ¯ [Trial 10/10] {'lr': 2e-05, 'batch_size': 32, 'dropout': 0.2, 'epochs': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [08:55<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.6853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1684/1684 [08:54<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 0.6765\n",
      "ğŸ“Š Acc: 0.5835 | Prec: 0.5757 | Rec: 0.9791 | F1: 0.7251\n",
      "ğŸ“Š Acc: 0.5878 | Prec: 0.5786 | Rec: 0.9794 | F1: 0.7275\n",
      "âœ… Val: 58.35% | Test: 58.78%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "best_val_acc = 0.0\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    # ğŸ”¹ ëœë¤ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„ íƒ\n",
    "    params = {k: random.choice(v) for k, v in search_space.items()}\n",
    "    print(f\"\\nğŸ¯ [Trial {trial+1}/{n_trials}] {params}\")\n",
    "\n",
    "    # ğŸ”¹ ëª¨ë¸ ì´ˆê¸°í™” (BERT freeze + headë§Œ í•™ìŠµ)\n",
    "    model = init_model(params[\"dropout\"])\n",
    "    train_loader = DataLoader(datasets_encoded[\"train\"], batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    val_loader   = DataLoader(datasets_encoded[\"validation\"], batch_size=64)\n",
    "    test_loader  = DataLoader(datasets_encoded[\"test\"], batch_size=64)\n",
    "\n",
    "    # ğŸ”¹ Optimizer & Scheduler êµ¬ì„±\n",
    "    optimizer, scheduler, criterion = configure_optimizer(\n",
    "        model, params[\"lr\"], params[\"epochs\"], train_loader\n",
    "    )\n",
    "\n",
    "    # ğŸ”¹ í•™ìŠµ ìˆ˜í–‰\n",
    "    train_model(model, optimizer, scheduler, train_loader, num_epochs=params[\"epochs\"])\n",
    "\n",
    "    # ğŸ”¹ Validation / Test í‰ê°€\n",
    "    val_metrics  = evaluate(model, val_loader)\n",
    "    test_metrics = evaluate(model, test_loader)\n",
    "    print(f\"âœ… Val: {val_metrics['acc']*100:.2f}% | Test: {test_metrics['acc']*100:.2f}%\")\n",
    "\n",
    "    # ğŸ”¹ ê²°ê³¼ ì €ì¥\n",
    "    results.append({\n",
    "        \"trial\": trial + 1,\n",
    "        \"lr\": params[\"lr\"],\n",
    "        \"batch_size\": params[\"batch_size\"],\n",
    "        \"dropout\": params[\"dropout\"],\n",
    "        \"epochs\": params[\"epochs\"],\n",
    "        \"val_acc\": val_metrics[\"acc\"],\n",
    "        \"val_prec\": val_metrics[\"prec\"],\n",
    "        \"val_rec\": val_metrics[\"rec\"],\n",
    "        \"val_f1\": val_metrics[\"f1\"],\n",
    "        \"test_acc\": test_metrics[\"acc\"],\n",
    "        \"test_prec\": test_metrics[\"prec\"],\n",
    "        \"test_rec\": test_metrics[\"rec\"],\n",
    "        \"test_f1\": test_metrics[\"f1\"]\n",
    "    })\n",
    "\n",
    "    # ğŸ”¹ Best ëª¨ë¸ ì—…ë°ì´íŠ¸ & ì €ì¥\n",
    "    if val_metrics['acc'] > best_val_acc:\n",
    "        best_val_acc = val_metrics['acc']\n",
    "        best_model   = model\n",
    "        best_params  = params\n",
    "\n",
    "        model.save_pretrained(\"./freeze_model\")\n",
    "        tokenizer.save_pretrained(\"./freeze_model\")\n",
    "\n",
    "        print(f\"ğŸ’¾ Best model updated! (Val: {best_val_acc*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6518b6",
   "metadata": {},
   "source": [
    "ğŸ“Š ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a842dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"freeze_finetune_random_search_tvt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff2ca2",
   "metadata": {},
   "source": [
    "ğŸ”¹ ìµœì¢… ê²°ê³¼ ìš”ì•½\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83c93e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Random Search Summary (Sorted by Validation Accuracy):\n",
      "   trial       lr  batch_size  dropout  epochs   val_acc  val_prec   val_rec  \\\n",
      "6      7  0.00010          32      0.3       4  0.733036  0.711087  0.882742   \n",
      "7      8  0.00010          16      0.3       3  0.725761  0.699442  0.896241   \n",
      "0      1  0.00010          64      0.3       4  0.696956  0.668347  0.912652   \n",
      "4      5  0.00003          16      0.3       4  0.656125  0.630536  0.934621   \n",
      "5      6  0.00010          64      0.2       2  0.651670  0.624349  0.951562   \n",
      "1      2  0.00005          64      0.1       3  0.638604  0.614754  0.952885   \n",
      "2      3  0.00002          32      0.1       3  0.593022  0.581615  0.978031   \n",
      "8      9  0.00003          64      0.3       3  0.588716  0.578554  0.982530   \n",
      "9     10  0.00002          32      0.2       2  0.583519  0.575720  0.979089   \n",
      "3      4  0.00001          16      0.3       2  0.560356  0.561903  0.981472   \n",
      "\n",
      "     val_f1  test_acc  test_prec  test_rec   test_f1  \n",
      "6  0.787671  0.740757   0.715921  0.892678  0.794588  \n",
      "7  0.785706  0.731403   0.702503  0.905102  0.791036  \n",
      "0  0.771624  0.698589   0.668590  0.918847  0.773992  \n",
      "4  0.753039  0.657906   0.630954  0.941845  0.755673  \n",
      "5  0.753985  0.656125   0.627587  0.953740  0.757029  \n",
      "1  0.747353  0.636971   0.613698  0.954533  0.747078  \n",
      "2  0.729444  0.598515   0.585028  0.981232  0.733017  \n",
      "8  0.728272  0.587082   0.577843  0.983082  0.727860  \n",
      "9  0.725081  0.587825   0.578635  0.979381  0.727469  \n",
      "3  0.714657  0.570898   0.567559  0.991541  0.721901  \n",
      "\n",
      "ğŸ† Best Trial Parameters:\n",
      "{'lr': 0.0001, 'batch_size': 32, 'dropout': 0.3, 'epochs': 4} Val Acc: 73.30%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ“Š Random Search Summary (Sorted by Validation Accuracy):\")\n",
    "print(df.sort_values(\"val_acc\", ascending=False))\n",
    "print(\"\\nğŸ† Best Trial Parameters:\")\n",
    "print(best_params, f\"Val Acc: {best_val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52317617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 1,538/109,483,778 (0.001405%)\n"
     ]
    }
   ],
   "source": [
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable:,}/{total:,} ({100*trainable/total:.6f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4d8f855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer params: 1,538\n"
     ]
    }
   ],
   "source": [
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(f\"Optimizer params: {sum(p.numel() for p in trainable_params):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17f22286",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(name, param.grad.abs().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "544ce33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f6663",
   "metadata": {},
   "source": [
    "best modle torchë¡œ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40f400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saved best model weights as freeze_model.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(best_model.state_dict(), \"freeze_model.pt\")\n",
    "print(\"ğŸ’¾ Saved best model weights as freeze_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
